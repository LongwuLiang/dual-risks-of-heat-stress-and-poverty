{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#重要！下载未来数据，2015-2100，ssp126，ssp245，ssp370，ssp585"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def download_chunk(url, start, end, save_dir, file_name, chunk_num, chunk_size, retries=3):\n",
    "    headers = {'Range': f'bytes={start}-{end}'}\n",
    "    chunk_path = os.path.join(save_dir, f\"{file_name}.part{chunk_num}\")\n",
    "\n",
    "    while retries > 0:\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, stream=True)\n",
    "            if response.status_code in [200, 206]:\n",
    "                content = response.content\n",
    "                if len(content) == chunk_size:\n",
    "                    with open(chunk_path, 'wb') as f:\n",
    "                        f.write(content)\n",
    "                    return True\n",
    "                else:\n",
    "                    print(f\"Chunk {chunk_num} size mismatch, expected {chunk_size}, got {len(content)}\")\n",
    "            else:\n",
    "                print(f\"Chunk {chunk_num} download failed with status code: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading chunk {chunk_num}: {e}\")\n",
    "\n",
    "        retries -= 1\n",
    "        print(f\"Retrying chunk {chunk_num}, attempt {3 - retries}\")\n",
    "\n",
    "    print(f\"Failed to download chunk {chunk_num} after multiple retries.\")\n",
    "    return False\n",
    "\n",
    "def download_file(url, save_dir, file_name, num_chunks=4, num_threads=20):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, file_name)\n",
    "\n",
    "    response = requests.head(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: Unable to access URL, status code {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    file_size = int(response.headers.get('content-length', 0))\n",
    "    chunk_size = file_size // num_chunks\n",
    "    last_chunk_size = file_size - chunk_size * (num_chunks - 1)\n",
    "\n",
    "    futures = []\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        for i in range(num_chunks):\n",
    "            start = chunk_size * i\n",
    "            end = start + chunk_size - 1 if i != num_chunks - 1 else file_size - 1\n",
    "            size = chunk_size if i != num_chunks - 1 else last_chunk_size\n",
    "            future = executor.submit(download_chunk, url, start, end, save_dir, file_name, i, size)\n",
    "            futures.append(future)\n",
    "\n",
    "    # 确保所有块都已正确下载\n",
    "    if not all(future.result() for future in as_completed(futures)):\n",
    "        print(f\"Not all chunks of {file_name} were downloaded successfully.\")\n",
    "        return\n",
    "\n",
    "    # 合并文件块\n",
    "    try:\n",
    "        with open(save_path, 'wb') as final_file:\n",
    "            for i in range(num_chunks):\n",
    "                part_file_path = os.path.join(save_dir, f\"{file_name}.part{i}\")\n",
    "                if os.path.exists(part_file_path):\n",
    "                    with open(part_file_path, 'rb') as part_file:\n",
    "                        final_file.write(part_file.read())\n",
    "                    os.remove(part_file_path)\n",
    "                else:\n",
    "                    print(f\"Missing chunk {i} for file {file_name}\")\n",
    "                    return\n",
    "    except Exception as e:\n",
    "        print(f\"Error while merging file {file_name}: {e}\")\n",
    "\n",
    "def download_yearly_data(data_type, start_year, end_year, model_name, scenario, num_threads=20):\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        file_name = f\"{data_type}_day_{model_name}_{scenario}_r1i1p1f1_gn_{year}.nc\"\n",
    "        base_url = f\"https://ds.nccs.nasa.gov/thredds/fileServer/AMES/NEX/GDDP-CMIP6/{model_name}/{scenario}/r1i1p1f1/{data_type}/{file_name}\"\n",
    "        save_dir = os.path.join(\"M:/GDDP-CMIP6\", model_name, scenario, data_type)\n",
    "        print(f\"Downloading {data_type} data for year: {year}\")\n",
    "        download_file(base_url, save_dir, file_name, num_threads=num_threads)\n",
    "\n",
    "\n",
    "# 下载 tas 数据\n",
    "download_yearly_data(\"tas\", 2092, 2100, \"CMCC-CM2-SR5\", \"ssp370\", num_threads=20)\n",
    "\n",
    "# 下载 tas 数据\n",
    "download_yearly_data(\"tas\", 2015, 2100, \"CMCC-CM2-SR5\", \"ssp585\", num_threads=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#重要！下载历史数据 1990-2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def download_chunk(url, start, end, save_dir, file_name, chunk_num, chunk_size, retries=3):\n",
    "    headers = {'Range': f'bytes={start}-{end}'}\n",
    "    chunk_path = os.path.join(save_dir, f\"{file_name}.part{chunk_num}\")\n",
    "\n",
    "    while retries > 0:\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, stream=True)\n",
    "            if response.status_code in [200, 206]:\n",
    "                content = response.content\n",
    "                if len(content) == chunk_size:\n",
    "                    with open(chunk_path, 'wb') as f:\n",
    "                        f.write(content)\n",
    "                    return True\n",
    "                else:\n",
    "                    print(f\"Chunk {chunk_num} size mismatch, expected {chunk_size}, got {len(content)}\")\n",
    "            else:\n",
    "                print(f\"Chunk {chunk_num} download failed with status code: {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading chunk {chunk_num}: {e}\")\n",
    "\n",
    "        retries -= 1\n",
    "        print(f\"Retrying chunk {chunk_num}, attempt {3 - retries}\")\n",
    "\n",
    "    print(f\"Failed to download chunk {chunk_num} after multiple retries.\")\n",
    "    return False\n",
    "\n",
    "def download_file(url, save_dir, file_name, num_chunks=4, num_threads=20):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    save_path = os.path.join(save_dir, file_name)\n",
    "\n",
    "    response = requests.head(url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: Unable to access URL, status code {response.status_code}\")\n",
    "        return\n",
    "\n",
    "    file_size = int(response.headers.get('content-length', 0))\n",
    "    chunk_size = file_size // num_chunks\n",
    "    last_chunk_size = file_size - chunk_size * (num_chunks - 1)\n",
    "\n",
    "    futures = []\n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        for i in range(num_chunks):\n",
    "            start = chunk_size * i\n",
    "            end = start + chunk_size - 1 if i != num_chunks - 1 else file_size - 1\n",
    "            size = chunk_size if i != num_chunks - 1 else last_chunk_size\n",
    "            future = executor.submit(download_chunk, url, start, end, save_dir, file_name, i, size)\n",
    "            futures.append(future)\n",
    "\n",
    "    # 确保所有块都已正确下载\n",
    "    if not all(future.result() for future in as_completed(futures)):\n",
    "        print(f\"Not all chunks of {file_name} were downloaded successfully.\")\n",
    "        return\n",
    "\n",
    "    # 合并文件块\n",
    "    try:\n",
    "        with open(save_path, 'wb') as final_file:\n",
    "            for i in range(num_chunks):\n",
    "                part_file_path = os.path.join(save_dir, f\"{file_name}.part{i}\")\n",
    "                if os.path.exists(part_file_path):\n",
    "                    with open(part_file_path, 'rb') as part_file:\n",
    "                        final_file.write(part_file.read())\n",
    "                    os.remove(part_file_path)\n",
    "                else:\n",
    "                    print(f\"Missing chunk {i} for file {file_name}\")\n",
    "                    return\n",
    "    except Exception as e:\n",
    "        print(f\"Error while merging file {file_name}: {e}\")\n",
    "\n",
    "def download_historical_data(data_type, start_year, end_year, model_name, num_threads=20):\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        file_name = f\"{data_type}_day_{model_name}_historical_r1i1p1f1_gn_{year}.nc\"\n",
    "        base_url = f\"https://ds.nccs.nasa.gov/thredds/fileServer/AMES/NEX/GDDP-CMIP6/{model_name}/historical/r1i1p1f1/{data_type}/{file_name}\"\n",
    "        save_dir = os.path.join(\"M:/GDDP-CMIP6\", model_name, \"historical\", data_type)\n",
    "        print(f\"Downloading {data_type} historical data for year: {year}\")\n",
    "        download_file(base_url, save_dir, file_name, num_threads=num_threads)\n",
    "\n",
    "def download_future_data(data_type, start_year, end_year, model_name, num_threads=20):\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        file_name = f\"{data_type}_day_{model_name}_r1i1p1f1_gn_{year}.nc\"\n",
    "        base_url = f\"https://ds.nccs.nasa.gov/thredds/fileServer/AMES/NEX/GDDP-CMIP6/{model_name}/r1i1p1f1/{data_type}/{file_name}\"\n",
    "        save_dir = os.path.join(\"M:/GDDP-CMIP6\", model_name, data_type)\n",
    "        print(f\"Downloading {data_type} future data for year: {year}\")\n",
    "        download_file(base_url, save_dir, file_name, num_threads=num_threads)\n",
    "\n",
    "# 下载 tas 数据（历史）\n",
    "download_historical_data(\"tas\", 1990, 2014, \"ACCESS-CM2\", num_threads=20)\n",
    "\n",
    "# 下载 tas 数据\n",
    "download_yearly_data(\"tas\", 2015, 2100, \"CMCC-CM2-SR5\", \"ssp585\", num_threads=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#重要！检查有问题的数据(历史)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 输入文件夹路径、输出文件夹路径、模式列表和情景\n",
    "input_folder = r\"L:\\GDDP-CMIP6\"\n",
    "scenarios =[\"historical\"]\n",
    "models = [\n",
    "    \"ACCESS-CM2\", \"ACCESS-ESM1-5\", \"CanESM5\", \n",
    "    \"CMCC-CM2-SR5\", \"CMCC-ESM2\", \"CNRM-CM6-1\", \"CNRM-ESM2-1\", \n",
    "    \"EC-Earth3\", \"EC-Earth3-Veg-LR\", \"FGOALS-g3\", \"GFDL-ESM4\", \n",
    "    \"GISS-E2-1-G\", \"INM-CM4-8\", \"INM-CM5-0\", \"IPSL-CM6A-LR\", \n",
    "    \"KACE-1-0-G\", \"MIROC6\", \"MIROC-ES2L\", \"MPI-ESM1-2-HR\", \n",
    "    \"MPI-ESM1-2-LR\", \"MRI-ESM2-0\", \"NorESM2-LM\", \"NorESM2-MM\", \n",
    "    \"TaiESM1\", \"UKESM1-0-LL\"\n",
    "]\n",
    "\n",
    "# 定义一个函数来检查文件是否可以被xarray打开\n",
    "def check_file(file_path):\n",
    "    try:\n",
    "        with xr.open_dataset(file_path, decode_times=False, engine='netcdf4'):\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "# 遍历每个情景、模型和年份，检查文件\n",
    "problematic_files = []\n",
    "for scenario in scenarios:\n",
    "    for model in tqdm(models):\n",
    "        for year in range(1950, 2015):\n",
    "            model_path = os.path.join(input_folder, model, scenario, \"tas\")\n",
    "\n",
    "            # 根据模型构建文件名\n",
    "            if model in [\"CNRM-CM6-1\", \"CNRM-ESM2-1\"]:\n",
    "                file_name_pattern = f\"tas_day_{model}_{scenario}_r1i1p1f2_gr_{year}.nc\"\n",
    "            elif model in [\"FGOALS-g3\"]:\n",
    "                file_name_pattern = f\"tas_day_{model}_{scenario}_r3i1p1f1_gn_{year}.nc\"\n",
    "            elif model in [\"GFDL-ESM4\", \"INM-CM4-8\", \"INM-CM5-0\"]:\n",
    "                file_name_pattern = f\"tas_day_{model}_{scenario}_r1i1p1f1_gr1_{year}.nc\"\n",
    "            elif model in [\"GISS-E2-1-G\", \"MIROC-ES2L\", \"UKESM1-0-LL\"]:\n",
    "                file_name_pattern = f\"tas_day_{model}_{scenario}_r1i1p1f2_gn_{year}.nc\"\n",
    "            elif model in [\"EC-Earth3\", \"EC-Earth3-Veg-LR\", \"IPSL-CM6A-LR\", \"KACE-1-0-G\"]:\n",
    "                file_name_pattern = f\"tas_day_{model}_{scenario}_r1i1p1f1_gr_{year}.nc\"\n",
    "            else:\n",
    "                file_name_pattern = f\"tas_day_{model}_{scenario}_r1i1p1f1_gn_{year}.nc\"\n",
    "\n",
    "            file_path = os.path.join(model_path, file_name_pattern)\n",
    "\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"File not found: {file_path}\")\n",
    "            elif not check_file(file_path):\n",
    "                problematic_files.append(file_path)\n",
    "\n",
    "# 输出有问题的文件列表\n",
    "print(\"Problematic files:\")\n",
    "for file in problematic_files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#重要！检查有问题的数据(未来，4情景)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 输入文件夹路径、输出文件夹路径、模式列表和情景\n",
    "input_folder = r\"L:\\GDDP-CMIP6\"\n",
    "scenarios = [\"ssp126\", \"ssp245\", \"ssp370\", \"ssp585\"]\n",
    "models = [\n",
    "    \"ACCESS-CM2\", \"ACCESS-ESM1-5\", \"BCC-CSM2-MR\", \"CanESM5\", \n",
    "    \"CMCC-CM2-SR5\", \"CMCC-ESM2\", \"CNRM-CM6-1\", \"CNRM-ESM2-1\", \n",
    "    \"EC-Earth3\", \"EC-Earth3-Veg-LR\", \"FGOALS-g3\", \"GFDL-ESM4\", \n",
    "    \"GISS-E2-1-G\", \"INM-CM4-8\", \"INM-CM5-0\", \"IPSL-CM6A-LR\", \n",
    "    \"KACE-1-0-G\", \"MIROC6\", \"MIROC-ES2L\", \"MPI-ESM1-2-HR\", \n",
    "    \"MPI-ESM1-2-LR\", \"MRI-ESM2-0\", \"NorESM2-LM\", \"NorESM2-MM\", \n",
    "    \"TaiESM1\", \"UKESM1-0-LL\"\n",
    "]\n",
    "\n",
    "# 定义一个函数来检查文件是否可以被xarray打开\n",
    "def check_file(file_path):\n",
    "    try:\n",
    "        with xr.open_dataset(file_path, decode_times=False, engine='netcdf4'):\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "\n",
    "# 遍历每个情景、模型和年份，检查文件\n",
    "problematic_files = []\n",
    "for scenario in scenarios:\n",
    "    for model in tqdm(models):\n",
    "        for year in range(2015, 2101):\n",
    "            model_path = os.path.join(input_folder, model, scenario, \"tas\")\n",
    "\n",
    "            # 根据模型构建文件名\n",
    "            if model in [\"CNRM-CM6-1\", \"CNRM-ESM2-1\"]:\n",
    "                file_name_pattern = f\"tas_day_{model}_{scenario}_r1i1p1f2_gr_{year}.nc\"\n",
    "            elif model in [\"FGOALS-g3\"]:\n",
    "                file_name_pattern = f\"tas_day_{model}_{scenario}_r3i1p1f1_gn_{year}.nc\"\n",
    "            elif model in [\"GFDL-ESM4\", \"INM-CM4-8\", \"INM-CM5-0\"]:\n",
    "                file_name_pattern = f\"tas_day_{model}_{scenario}_r1i1p1f1_gr1_{year}.nc\"\n",
    "            elif model in [\"GISS-E2-1-G\", \"MIROC-ES2L\", \"UKESM1-0-LL\"]:\n",
    "                file_name_pattern = f\"tas_day_{model}_{scenario}_r1i1p1f2_gn_{year}.nc\"\n",
    "            elif model in [\"EC-Earth3\", \"EC-Earth3-Veg-LR\", \"IPSL-CM6A-LR\", \"KACE-1-0-G\"]:\n",
    "                file_name_pattern = f\"tas_day_{model}_{scenario}_r1i1p1f1_gr_{year}.nc\"\n",
    "            else:\n",
    "                file_name_pattern = f\"tas_day_{model}_{scenario}_r1i1p1f1_gn_{year}.nc\"\n",
    "\n",
    "            file_path = os.path.join(model_path, file_name_pattern)\n",
    "\n",
    "            if not os.path.exists(file_path):\n",
    "                print(f\"File not found: {file_path}\")\n",
    "            elif not check_file(file_path):\n",
    "                problematic_files.append(file_path)\n",
    "\n",
    "# 输出有问题的文件列表\n",
    "print(\"Problematic files:\")\n",
    "for file in problematic_files:\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#这段代码是可以用的，计算了历史月度均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 定义情景和年份\n",
    "scenarios = ['historical']\n",
    "years = list(range(1990, 2015))\n",
    "\n",
    "# 处理每个情景和年份\n",
    "for scenario in scenarios:\n",
    "    for year in years:\n",
    "        # 构建文件路径\n",
    "        input_dir = f'L:/CMIP6-1final/{scenario}/'\n",
    "        input_file = f'tas_day_{scenario}_r1i1p1f1_gn_{year}.nc'\n",
    "        file_path = os.path.join(input_dir, input_file)\n",
    "\n",
    "        # 创建输出文件夹\n",
    "        output_dir = f'L:/CMIP6-1yue/{scenario}/'\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        # 打开数据集\n",
    "        data = xr.open_dataset(file_path)\n",
    "\n",
    "        # 计算每个月的平均值\n",
    "        monthly_mean = data.resample(time='1M').mean(dim='time', skipna=True)\n",
    "\n",
    "        # 修改时间维度的格式\n",
    "        monthly_mean['time'] = np.arange(12)\n",
    "\n",
    "        # 保存月平均数据到NetCDF文件\n",
    "        output_file = f'tas_day_{scenario}_r1i1p1f1_gn_{year}_month.nc'\n",
    "        output_file_path = os.path.join(output_dir, output_file)\n",
    "        monthly_mean.to_netcdf(output_file_path)\n",
    "\n",
    "        print(f'Processed {scenario} for year {year}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#这段代码是可以用的，计算了未来月度均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 定义情景和年份\n",
    "scenarios = ['ssp126', 'ssp245', 'ssp370', 'ssp585']\n",
    "years = list(range(2015, 2030)) + list(range(2030, 2101, 10))\n",
    "\n",
    "# 遍历情景和年份\n",
    "for scenario in scenarios:\n",
    "    for year in years:\n",
    "        # 构建文件路径\n",
    "        input_file = f'L:/CMIP6-1final/{scenario}/tas_day_{scenario}_r1i1p1f1_gn_{year}.nc'\n",
    "        output_dir = f'L:/CMIP6-1yue/{scenario}/'\n",
    "        output_file = f'{output_dir}tas_day_{scenario}_r1i1p1f1_gn_{year}_month.nc'\n",
    "\n",
    "        # 创建目标文件夹\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        # 打开数据集\n",
    "        data = xr.open_dataset(input_file)\n",
    "\n",
    "        # 计算每个月的平均值\n",
    "        monthly_mean = data.resample(time='1M').mean(dim='time', skipna=True)\n",
    "\n",
    "        # 将温度转换为摄氏度\n",
    "        monthly_mean -= 273.15\n",
    "\n",
    "        # 修改时间维度的格式\n",
    "        monthly_mean['time'] = np.arange(12)\n",
    "\n",
    "        # 保存月平均数据到NetCDF文件\n",
    "        monthly_mean.to_netcdf(output_file)\n",
    "\n",
    "        # 关闭数据集\n",
    "        data.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#重要！这个不是计算过程！在计算26个模式每日均值时，还会遇到问题returned non-zero exit status 1，用下面代码可以检查出有问题的数据文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "import uuid\n",
    "\n",
    "def is_cdo_available():\n",
    "    try:\n",
    "        subprocess.run([\"wsl\", \"cdo\", \"-V\"], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        return True\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        return False\n",
    "\n",
    "if not is_cdo_available():\n",
    "    print(\"CDO is not installed or not found in PATH in WSL. Please install CDO in WSL and ensure it is in the PATH.\")\n",
    "    exit()\n",
    "\n",
    "# 输入文件夹的Windows路径\n",
    "input_folder = \"L:/GDDP-CMIP6\"\n",
    "\n",
    "# 模型列表\n",
    "models = [\n",
    "    \"ACCESS-CM2\", \"ACCESS-ESM1-5\", \"BCC-CSM2-MR\", \"CanESM5\", \n",
    "    \"CMCC-CM2-SR5\", \"CMCC-ESM2\", \"CNRM-CM6-1\", \"CNRM-ESM2-1\", \n",
    "    \"EC-Earth3\", \"EC-Earth3-Veg-LR\", \"FGOALS-g3\", \"GFDL-ESM4\", \n",
    "    \"GISS-E2-1-G\", \"INM-CM4-8\", \"INM-CM5-0\", \"IPSL-CM6A-LR\", \n",
    "    \"KACE-1-0-G\", \"MIROC6\", \"MIROC-ES2L\", \"MPI-ESM1-2-HR\", \n",
    "    \"MPI-ESM1-2-LR\", \"MRI-ESM2-0\", \"NorESM2-LM\", \"NorESM2-MM\", \n",
    "    \"TaiESM1\", \"UKESM1-0-LL\"\n",
    "]\n",
    "\n",
    "# 情景和年份\n",
    "#scenario = \"historical\"\n",
    "#year = 2010\n",
    "\n",
    "# 情景列表\n",
    "scenarios = [\"ssp126\", \"ssp245\", \"ssp370\", \"ssp585\"]\n",
    "\n",
    "# 年份范围\n",
    "years = range(2015, 2101)\n",
    "\n",
    "# 构建文件路径\n",
    "files_to_check = []\n",
    "for model in models:\n",
    "    if model in [\"CNRM-CM6-1\", \"CNRM-ESM2-1\"]:\n",
    "        file_name = f\"tas_day_{model}_{scenario}_r1i1p1f2_gr_{year}.nc\"\n",
    "    elif model in [\"FGOALS-g3\"]:\n",
    "        file_name = f\"tas_day_{model}_{scenario}_r3i1p1f1_gn_{year}.nc\"\n",
    "    elif model in [\"GFDL-ESM4\", \"INM-CM4-8\", \"INM-CM5-0\"]:\n",
    "        file_name = f\"tas_day_{model}_{scenario}_r1i1p1f1_gr1_{year}.nc\"\n",
    "    elif model in [\"GISS-E2-1-G\", \"MIROC-ES2L\", \"UKESM1-0-LL\"]:\n",
    "        file_name = f\"tas_day_{model}_{scenario}_r1i1p1f2_gn_{year}.nc\"\n",
    "    elif model in [\"EC-Earth3\", \"EC-Earth3-Veg-LR\", \"IPSL-CM6A-LR\", \"KACE-1-0-G\"]:\n",
    "        file_name = f\"tas_day_{model}_{scenario}_r1i1p1f1_gr_{year}.nc\"\n",
    "    else:\n",
    "        file_name = f\"tas_day_{model}_{scenario}_r1i1p1f1_gn_{year}.nc\"\n",
    "\n",
    "    file_path = f\"L:/GDDP-CMIP6/{model}/{scenario}/tas/{file_name}\"\n",
    "    files_to_check.append(file_path)\n",
    "\n",
    "# 检查每个文件\n",
    "for file in tqdm(files_to_check):\n",
    "    temp_output_file = f\"/mnt/l/{uuid.uuid4()}.nc\"  # 使用UUID生成唯一的临时文件名\n",
    "    wsl_file_path = file.replace(\"L:\", \"/mnt/l\").replace(\"\\\\\", \"/\")\n",
    "    try:\n",
    "        subprocess.run([\"wsl\", \"cdo\", \"ensmean\", wsl_file_path, temp_output_file], check=True, stderr=subprocess.PIPE)\n",
    "        print(f\"File processed successfully: {file}\")\n",
    "        os.remove(temp_output_file.replace(\"/mnt/l\", \"L:\"))  # 删除生成的临时文件\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error processing file: {file}\")\n",
    "        print(f\"Error message: {e.stderr.decode()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#不要的！计算9个模式每日均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 设置输入文件夹路径、输出文件夹路径、模式列表和情景\n",
    "input_folder = r\"L:\\GDDP-CMIP6\"\n",
    "output_folder = r\"L:\\CMIP6\\ssp585\"\n",
    "scenario = \"ssp585\"\n",
    "models = [\n",
    "    \"ACCESS-CM2\",\n",
    "    \"BCC-CSM2-MR\",\n",
    "    \"CanESM5\",\n",
    "    \"CMCC-CM2-SR5\",\n",
    "    \"CMCC-ESM2\",\n",
    "    \"FGOALS-g3\",\n",
    "    \"GISS-E2-1-G\",\n",
    "    \"KACE-1-0-G\",\n",
    "    \"MRI-ESM2-0\"\n",
    "]\n",
    "\n",
    "# 创建输出文件夹\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# 遍历每个年份\n",
    "for year in tqdm(range(2015, 2101)):\n",
    "    # 创建一个空的 xarray 数据集来存储每天的模式均值数据\n",
    "    daily_means = []\n",
    "\n",
    "    # 遍历每个模型\n",
    "    for model in models:\n",
    "        # 获取模型文件夹路径\n",
    "        model_path = os.path.join(input_folder, model, scenario)\n",
    "\n",
    "        # 获取文件名\n",
    "        # 根据模型不同的文件命名方式，使用不同的匹配规则\n",
    "        if model == \"FGOALS-g3\":\n",
    "            file_name_pattern = f\"tasmax_day_{model}_{scenario}_r3i1p1f1_gn_{year}.nc\"\n",
    "        elif model == \"GISS-E2-1-G\":\n",
    "            file_name_pattern = f\"tasmax_day_{model}_{scenario}_r1i1p1f2_gn_{year}.nc\"\n",
    "        elif model == \"KACE-1-0-G\":\n",
    "            file_name_pattern = f\"tasmax_day_{model}_{scenario}_r1i1p1f1_gr_{year}.nc\"\n",
    "        else:\n",
    "            file_name_pattern = f\"tasmax_day_{model}_{scenario}_r1i1p1f1_gn_{year}.nc\"\n",
    "\n",
    "        # 构建完整文件路径\n",
    "        file_path = os.path.join(model_path, file_name_pattern)\n",
    "\n",
    "        # 确保文件存在\n",
    "        if os.path.exists(file_path):\n",
    "            # 读取数据并明确指定日历类型\n",
    "            ds = xr.open_dataset(file_path, decode_times=False, engine='netcdf4')\n",
    "            ds['time'] = xr.cftime_range(str(year), periods=len(ds['time']), freq='D')\n",
    "            ds = xr.decode_cf(ds)\n",
    "\n",
    "            # 将时间维度设为 'date'\n",
    "            ds = ds.rename({'time': 'date'})\n",
    "\n",
    "            # 将数据添加到列表中\n",
    "            daily_means.append(ds['tasmax'])\n",
    "        else:\n",
    "            print(f\"File not found for {model} in year {year}. Skipping for this model.\")\n",
    "\n",
    "    if daily_means:\n",
    "        # 合并所有模型的数据\n",
    "        combined_data = xr.concat(daily_means, dim=\"model\")\n",
    "\n",
    "        # 计算每个日期的模式均值\n",
    "        daily_model_means = combined_data.mean(dim='model', skipna=True)\n",
    "\n",
    "        # 按照日期先后顺序排序\n",
    "        daily_model_means = daily_model_means.sortby('date')\n",
    "\n",
    "        # 保存结果到文件\n",
    "        output_file = os.path.join(output_folder, f\"tasmax_day_ssp585_r1i1p1f1_gn_{year}.nc\")\n",
    "        daily_model_means.to_netcdf(output_file)\n",
    "\n",
    "        print(f\"Year {year} processed and saved.\")\n",
    "    else:\n",
    "        print(f\"No data found for year {year}. Skipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#重要！这个还有问题！计算26个模式每日均值，每年大概需要12分钟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 设置输入文件夹路径、输出文件夹路径、模式列表和情景\n",
    "input_folder = r\"L:\\GDDP-CMIP6\"\n",
    "output_folder = r\"L:\\CMIP6-1\\ssp126\"\n",
    "scenario = \"ssp126\"\n",
    "models = [\n",
    "    \"ACCESS-CM2\", \"ACCESS-ESM1-5\", \"BCC-CSM2-MR\", \"CanESM5\", \n",
    "    \"CMCC-CM2-SR5\", \"CMCC-ESM2\", \"CNRM-CM6-1\", \"CNRM-ESM2-1\", \n",
    "    \"EC-Earth3\", \"EC-Earth3-Veg-LR\", \"FGOALS-g3\", \"GFDL-ESM4\", \n",
    "    \"GISS-E2-1-G\", \"INM-CM4-8\", \"INM-CM5-0\", \"IPSL-CM6A-LR\", \n",
    "    \"KACE-1-0-G\", \"MIROC6\", \"MIROC-ES2L\", \"MPI-ESM1-2-HR\", \n",
    "    \"MPI-ESM1-2-LR\", \"MRI-ESM2-0\", \"NorESM2-LM\", \"NorESM2-MM\", \n",
    "    \"TaiESM1\", \"UKESM1-0-LL\"\n",
    "]\n",
    "\n",
    "# 创建输出文件夹\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# 遍历每个年份\n",
    "for year in tqdm(range(2081, 2101)):\n",
    "    # 创建一个空的 xarray 数据集来存储每天的模式均值数据\n",
    "    daily_means = []\n",
    "\n",
    "    # 遍历每个模型\n",
    "    for model in models:\n",
    "        # 获取模型文件夹路径\n",
    "        model_path = os.path.join(input_folder, model, scenario, \"tas\")\n",
    "\n",
    "        # 构建文件名\n",
    "        if model in [\"CNRM-CM6-1\", \"CNRM-ESM2-1\"]:\n",
    "            file_name_pattern = f\"tas_day_{model}_{scenario}_r1i1p1f2_gr_{year}.nc\"\n",
    "        elif model in [\"FGOALS-g3\"]:\n",
    "            file_name_pattern = f\"tas_day_{model}_{scenario}_r3i1p1f1_gn_{year}.nc\"\n",
    "        elif model in [\"GFDL-ESM4\", \"INM-CM4-8\", \"INM-CM5-0\"]:\n",
    "            file_name_pattern = f\"tas_day_{model}_{scenario}_r1i1p1f1_gr1_{year}.nc\"\n",
    "        elif model in [\"GISS-E2-1-G\", \"MIROC-ES2L\", \"UKESM1-0-LL\"]:\n",
    "            file_name_pattern = f\"tas_day_{model}_{scenario}_r1i1p1f2_gn_{year}.nc\"\n",
    "        elif model in [\"EC-Earth3\", \"EC-Earth3-Veg-LR\", \"IPSL-CM6A-LR\", \"KACE-1-0-G\"]:\n",
    "            file_name_pattern = f\"tas_day_{model}_{scenario}_r1i1p1f1_gr_{year}.nc\"\n",
    "        else:\n",
    "            file_name_pattern = f\"tas_day_{model}_{scenario}_r1i1p1f1_gn_{year}.nc\"\n",
    "\n",
    "\n",
    "       # 构建完整文件路径\n",
    "        file_path = os.path.join(model_path, file_name_pattern)\n",
    "\n",
    "        # 确保文件存在\n",
    "        if os.path.exists(file_path):\n",
    "            # 读取数据并明确指定日历类型\n",
    "            ds = xr.open_dataset(file_path, decode_times=False, engine='netcdf4')\n",
    "            ds['time'] = xr.cftime_range(str(year), periods=len(ds['time']), freq='D')\n",
    "            ds = xr.decode_cf(ds)\n",
    "\n",
    "            # 将时间维度设为 'date'\n",
    "            ds = ds.rename({'time': 'date'})\n",
    "\n",
    "            # 将数据添加到列表中\n",
    "            daily_means.append(ds['tas'])\n",
    "        else:\n",
    "            print(f\"File not found for {model} in year {year}. Skipping for this model.\")\n",
    "\n",
    "    if daily_means:\n",
    "        # 合并所有模型的数据\n",
    "        combined_data = xr.concat(daily_means, dim=\"model\")\n",
    "\n",
    "        # 计算每个日期的模式均值\n",
    "        daily_model_means = combined_data.mean(dim='model', skipna=True)\n",
    "\n",
    "        # 按照日期先后顺序排序\n",
    "        daily_model_means = daily_model_means.sortby('date')\n",
    "\n",
    "        # 保存结果到文件\n",
    "        output_file = os.path.join(output_folder, f\"tas_day_ssp126_r1i1p1f1_gn_{year}.nc\")\n",
    "        daily_model_means.to_netcdf(output_file)\n",
    "\n",
    "        print(f\"Year {year} processed and saved.\")\n",
    "    else:\n",
    "        print(f\"No data found for year {year}. Skipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#重要！检查原始数据是否缺失部分日期数据,12月"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def check_for_december_27st(input_folder, models, scenarios, year):\n",
    "    \"\"\"\n",
    "    检查指定模型、情景和特定年份的NetCDF文件是否包含12月27日的数据。\n",
    "    \"\"\"\n",
    "    for model in models:\n",
    "        for scenario in scenarios:\n",
    "            # 构建文件路径\n",
    "            model_path = os.path.join(input_folder, model, scenario, \"tas\")\n",
    "\n",
    "            # 根据模型和情景构建文件名\n",
    "            if model in [\"CNRM-CM6-1\", \"CNRM-ESM2-1\"]:\n",
    "                file_name = f\"tas_day_{model}_{scenario}_r1i1p1f2_gr_{year}.nc\"\n",
    "            elif model in [\"FGOALS-g3\"]:\n",
    "                file_name = f\"tas_day_{model}_{scenario}_r3i1p1f1_gn_{year}.nc\"\n",
    "            elif model in [\"GFDL-ESM4\", \"INM-CM4-8\", \"INM-CM5-0\"]:\n",
    "                file_name = f\"tas_day_{model}_{scenario}_r1i1p1f1_gr1_{year}.nc\"\n",
    "            elif model in [\"GISS-E2-1-G\", \"MIROC-ES2L\", \"UKESM1-0-LL\"]:\n",
    "                file_name = f\"tas_day_{model}_{scenario}_r1i1p1f2_gn_{year}.nc\"\n",
    "            elif model in [\"EC-Earth3\", \"EC-Earth3-Veg-LR\", \"IPSL-CM6A-LR\", \"KACE-1-0-G\"]:\n",
    "                file_name = f\"tas_day_{model}_{scenario}_r1i1p1f1_gr_{year}.nc\"\n",
    "            else:\n",
    "                file_name = f\"tas_day_{model}_{scenario}_r1i1p1f1_gn_{year}.nc\"\n",
    "\n",
    "            file_path = os.path.join(model_path, file_name)\n",
    "            wsl_file_path = file_path.replace(\"L:\", \"/mnt/l\").replace(\"\\\\\", \"/\")\n",
    "\n",
    "            if os.path.exists(file_path):\n",
    "                try:\n",
    "                    # 使用CDO的showdate命令显示文件中的日期\n",
    "                    result = subprocess.run([\"wsl\", \"cdo\", \"showdate\", wsl_file_path], \n",
    "                                            stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True, check=True)\n",
    "                    dates = result.stdout.strip().split()\n",
    "                    december_27st = f\"{year}-12-27\"\n",
    "                    if december_27st in dates:\n",
    "                        print(f\"December 27st is present in {file_path}\")\n",
    "                    else:\n",
    "                        print(f\"December 27st is missing in {file_path}\")\n",
    "                except subprocess.CalledProcessError as e:\n",
    "                    print(f\"Error processing file {file_path}: {e}\")\n",
    "            else:\n",
    "                print(f\"File not found: {file_path}\")\n",
    "\n",
    "# 输入文件夹的Windows路径\n",
    "input_folder = \"L:/GDDP-CMIP6\"\n",
    "\n",
    "# 模型列表\n",
    "models = [\n",
    "    \"ACCESS-CM2\", \"ACCESS-ESM1-5\", \"BCC-CSM2-MR\", \"CanESM5\", \n",
    "    \"CMCC-CM2-SR5\", \"CMCC-ESM2\", \"CNRM-CM6-1\", \"CNRM-ESM2-1\", \n",
    "    \"EC-Earth3\", \"EC-Earth3-Veg-LR\", \"FGOALS-g3\", \"GFDL-ESM4\", \n",
    "    \"GISS-E2-1-G\", \"INM-CM4-8\", \"INM-CM5-0\", \"IPSL-CM6A-LR\", \n",
    "    \"KACE-1-0-G\", \"MIROC6\", \"MIROC-ES2L\", \"MPI-ESM1-2-HR\", \n",
    "    \"MPI-ESM1-2-LR\", \"MRI-ESM2-0\", \"NorESM2-LM\", \"NorESM2-MM\", \n",
    "    \"TaiESM1\", \"UKESM1-0-LL\"\n",
    "]\n",
    "\n",
    "# 情景列表\n",
    "scenarios = [\"ssp126\", \"ssp245\", \"ssp370\", \"ssp585\"]\n",
    "\n",
    "# 只检查2025年的数据\n",
    "check_for_december_27st(input_folder, models, scenarios, 2100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#重要！检查原始数据是否缺失部分日期数据,2月29日"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def check_for_february_29(input_folder, models, scenarios, year):\n",
    "    \"\"\"\n",
    "    检查指定模型、情景和特定年份的NetCDF文件是否包含2月29日的数据。\n",
    "    \"\"\"\n",
    "    for model in models:\n",
    "        for scenario in scenarios:\n",
    "            # 构建文件路径\n",
    "            model_path = os.path.join(input_folder, model, scenario, \"tas\")\n",
    "\n",
    "            # 根据模型和情景构建文件名\n",
    "            if model in [\"CNRM-CM6-1\", \"CNRM-ESM2-1\"]:\n",
    "                file_name = f\"tas_day_{model}_{scenario}_r1i1p1f2_gr_{year}.nc\"\n",
    "            elif model in [\"FGOALS-g3\"]:\n",
    "                file_name = f\"tas_day_{model}_{scenario}_r3i1p1f1_gn_{year}.nc\"\n",
    "            elif model in [\"GFDL-ESM4\", \"INM-CM4-8\", \"INM-CM5-0\"]:\n",
    "                file_name = f\"tas_day_{model}_{scenario}_r1i1p1f1_gr1_{year}.nc\"\n",
    "            elif model in [\"GISS-E2-1-G\", \"MIROC-ES2L\", \"UKESM1-0-LL\"]:\n",
    "                file_name = f\"tas_day_{model}_{scenario}_r1i1p1f2_gn_{year}.nc\"\n",
    "            elif model in [\"EC-Earth3\", \"EC-Earth3-Veg-LR\", \"IPSL-CM6A-LR\", \"KACE-1-0-G\"]:\n",
    "                file_name = f\"tas_day_{model}_{scenario}_r1i1p1f1_gr_{year}.nc\"\n",
    "            else:\n",
    "                file_name = f\"tas_day_{model}_{scenario}_r1i1p1f1_gn_{year}.nc\"\n",
    "\n",
    "            file_path = os.path.join(model_path, file_name)\n",
    "            wsl_file_path = file_path.replace(\"L:\", \"/mnt/l\").replace(\"\\\\\", \"/\")\n",
    "\n",
    "            if os.path.exists(file_path):\n",
    "                try:\n",
    "                    # 使用CDO的showdate命令显示文件中的日期\n",
    "                    result = subprocess.run([\"wsl\", \"cdo\", \"showdate\", wsl_file_path], \n",
    "                                            stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True, check=True)\n",
    "                    dates = result.stdout.strip().split()\n",
    "                    february_29 = f\"{year}-02-29\"\n",
    "                    if february_29 in dates:\n",
    "                        print(f\"February 29 is present in {file_path}\")\n",
    "                    else:\n",
    "                        print(f\"February 29 is missing in {file_path}\")\n",
    "                except subprocess.CalledProcessError as e:\n",
    "                    print(f\"Error processing file {file_path}: {e}\")\n",
    "            else:\n",
    "                print(f\"File not found: {file_path}\")\n",
    "\n",
    "# 输入文件夹的Windows路径\n",
    "input_folder = \"L:/GDDP-CMIP6\"\n",
    "\n",
    "# 模型列表\n",
    "models = [\n",
    "    \"ACCESS-CM2\", \"ACCESS-ESM1-5\", \"BCC-CSM2-MR\", \"CanESM5\", \n",
    "    \"CMCC-CM2-SR5\", \"CMCC-ESM2\", \"CNRM-CM6-1\", \"CNRM-ESM2-1\", \n",
    "    \"EC-Earth3\", \"EC-Earth3-Veg-LR\", \"FGOALS-g3\", \"GFDL-ESM4\", \n",
    "    \"GISS-E2-1-G\", \"INM-CM4-8\", \"INM-CM5-0\", \"IPSL-CM6A-LR\", \n",
    "    \"KACE-1-0-G\", \"MIROC6\", \"MIROC-ES2L\", \"MPI-ESM1-2-HR\", \n",
    "    \"MPI-ESM1-2-LR\", \"MRI-ESM2-0\", \"NorESM2-LM\", \"NorESM2-MM\", \n",
    "    \"TaiESM1\", \"UKESM1-0-LL\"\n",
    "]\n",
    "\n",
    "# 情景列表\n",
    "scenarios = [\"ssp126\", \"ssp245\", \"ssp370\", \"ssp585\"]\n",
    "\n",
    "# 只检查2020年的数据\n",
    "check_for_february_29(input_folder, models, scenarios, 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#重要！这个是最终的\n",
    "#KACE-1-0-G和UKESM1-0-LL两个模式的最长日期没有31日（1月、3月、5月、7月、8月、10月和12月都没有），导致计算有误，不要这两个模式，\n",
    "#没有2月29日的原始数据模式包括BCC-CSM2-MR、CanESM5、CMCC-CM2-SR5、CMCC-ESM2、FGOALS-g3、GFDL-ESM4、GISS-E2-1-G、INM-CM4-8、INM-CM5-0、NorESM2-LM、NorESM2-MM、TaiESM1\n",
    "#bcc-csm2-mr这个模式没有湿度数据，考虑到后面要用到湿度数据，也不要这个模式\n",
    "#这个不能同一台电脑运行多个界面，否则速度将至不到三分之一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "def is_date_present_in_file(wsl_file_path, date_to_check):\n",
    "    \"\"\"\n",
    "    检查指定的WSL路径下的文件是否包含特定日期。\n",
    "    使用 CDO showdate 命令来检查。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"wsl\", \"cdo\", \"showdate\", wsl_file_path],\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            universal_newlines=True,\n",
    "            check=True,\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        dates = result.stdout.strip().split()\n",
    "        return date_to_check in dates\n",
    "    except subprocess.CalledProcessError:\n",
    "        return False\n",
    "\n",
    "def is_cdo_available():\n",
    "    \"\"\"\n",
    "    检查CDO工具是否在WSL中可用。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        subprocess.run([\"wsl\", \"cdo\", \"-V\"], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        return True\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        return False\n",
    "\n",
    "if not is_cdo_available():\n",
    "    print(\"cdo is not installed or not found in PATH in WSL. Please install cdo in WSL and ensure it is in the PATH.\")\n",
    "    exit()\n",
    "\n",
    "# 输入文件夹的Windows路径\n",
    "input_folder = \"L:/GDDP-CMIP6\"\n",
    "\n",
    "# 模型列表\n",
    "models = [\n",
    "    \"ACCESS-CM2\", \"ACCESS-ESM1-5\", \"CanESM5\", \n",
    "    \"CMCC-CM2-SR5\", \"CMCC-ESM2\", \"CNRM-CM6-1\", \"CNRM-ESM2-1\", \n",
    "    \"EC-Earth3\", \"EC-Earth3-Veg-LR\", \"FGOALS-g3\", \"GFDL-ESM4\", \n",
    "    \"GISS-E2-1-G\", \"INM-CM4-8\", \"INM-CM5-0\", \"IPSL-CM6A-LR\", \n",
    "    \"KACE-1-0-G\", \"MIROC6\", \"MIROC-ES2L\", \"MPI-ESM1-2-HR\", \n",
    "    \"MPI-ESM1-2-LR\", \"MRI-ESM2-0\", \"NorESM2-LM\", \"NorESM2-MM\", \n",
    "    \"TaiESM1\", \"UKESM1-0-LL\"\n",
    "]\n",
    "\n",
    "# 情景和对应年份范围的字典\n",
    "scenarios_years = {\n",
    "    \"ssp126\": range(2015, 2101),\n",
    "    \"ssp245\": range(2015, 2101),\n",
    "    \"ssp370\": range(2015, 2101),\n",
    "    \"ssp585\": range(2015, 2101)\n",
    "}\n",
    "\n",
    "# 特定日期缺失的模型字典\n",
    "models_missing_dates = {\n",
    "    \"February_29\": [\"CanESM5\", \"CMCC-CM2-SR5\", \"CMCC-ESM2\", \"FGOALS-g3\", \"GFDL-ESM4\", \"GISS-E2-1-G\", \"INM-CM4-8\", \"INM-CM5-0\", \"NorESM2-LM\", \"NorESM2-MM\", \"TaiESM1\"],\n",
    "    \"End_of_Month_31\": [\"KACE-1-0-G\", \"UKESM1-0-LL\"]\n",
    "}\n",
    "\n",
    "# 遍历每个情景和其对应的年份范围\n",
    "for scenario, years in scenarios_years.items():\n",
    "    output_folder = f\"L:/CMIP6-1final/{scenario}\"\n",
    "\n",
    "    # 创建输出文件夹（如果不存在）\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for year in tqdm(years):\n",
    "        start_time = datetime.now()  # 开始时间\n",
    "        files_to_process = []\n",
    "\n",
    "        # 特定日期检查\n",
    "        february_29 = f\"{year}-02-29\"\n",
    "        months_with_31_days = [1, 3, 5, 7, 8, 10, 12]\n",
    "\n",
    "        # 遍历所有模型\n",
    "        for model in models:\n",
    "            model_path = os.path.join(input_folder, model, scenario, \"tas\")\n",
    "            \n",
    "            # 构建完整的文件名和路径\n",
    "            if model in [\"CNRM-CM6-1\", \"CNRM-ESM2-1\"]:\n",
    "                file_name = f\"tas_day_{model}_{scenario}_r1i1p1f2_gr_{year}.nc\"\n",
    "            elif model in [\"FGOALS-g3\"]:\n",
    "                file_name = f\"tas_day_{model}_{scenario}_r3i1p1f1_gn_{year}.nc\"\n",
    "            elif model in [\"GFDL-ESM4\", \"INM-CM4-8\", \"INM-CM5-0\"]:\n",
    "                file_name = f\"tas_day_{model}_{scenario}_r1i1p1f1_gr1_{year}.nc\"\n",
    "            elif model in [\"GISS-E2-1-G\", \"MIROC-ES2L\", \"UKESM1-0-LL\"]:\n",
    "                file_name = f\"tas_day_{model}_{scenario}_r1i1p1f2_gn_{year}.nc\"\n",
    "            elif model in [\"EC-Earth3\", \"EC-Earth3-Veg-LR\", \"IPSL-CM6A-LR\", \"KACE-1-0-G\"]:\n",
    "                file_name = f\"tas_day_{model}_{scenario}_r1i1p1f1_gr_{year}.nc\"\n",
    "            else:\n",
    "                file_name = f\"tas_day_{model}_{scenario}_r1i1p1f1_gn_{year}.nc\"\n",
    "                \n",
    "            file_path = os.path.join(model_path, file_name)\n",
    "            wsl_file_path = file_path.replace(\"L:\", \"/mnt/l\").replace(\"\\\\\", \"/\")\n",
    "\n",
    "            if os.path.exists(file_path):\n",
    "                include_file = True\n",
    "\n",
    "                # 检查特定模型是否缺失2月29日\n",
    "                if model in models_missing_dates[\"February_29\"]:\n",
    "                    if not is_date_present_in_file(wsl_file_path, february_29):\n",
    "                        include_file = False\n",
    "\n",
    "                # 检查特定模型是否缺失包含31日的月份\n",
    "                if model in models_missing_dates[\"End_of_Month_31\"]:\n",
    "                    for month in months_with_31_days:\n",
    "                        last_day = f\"{year}-{month:02d}-31\"\n",
    "                        if not is_date_present_in_file(wsl_file_path, last_day):\n",
    "                            include_file = False\n",
    "                            break\n",
    "\n",
    "                if include_file:\n",
    "                    files_to_process.append(wsl_file_path)\n",
    "\n",
    "        # 处理文件并计算均值\n",
    "        if files_to_process:\n",
    "            output_file = os.path.join(output_folder, f\"tas_day_{scenario}_r1i1p1f1_gn_{year}.nc\")\n",
    "            wsl_output_file = output_file.replace(\"L:\", \"/mnt/l\").replace(\"\\\\\", \"/\")\n",
    "\n",
    "            cdo_command = [\"wsl\", \"cdo\", \"ensmean\"] + files_to_process + [wsl_output_file]\n",
    "\n",
    "            try:\n",
    "                subprocess.run(cdo_command, check=True)\n",
    "                print(f\"Processed and saved data for scenario {scenario}, year {year}\")\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"Error processing data for scenario {scenario}, year {year}: {e}\")\n",
    "\n",
    "            end_time = datetime.now()  # 结束时间\n",
    "            elapsed_time = end_time - start_time\n",
    "            print(f\"Time taken for scenario {scenario}, year {year}: {elapsed_time}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"No data found for scenario {scenario}, year {year}. Skipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#KACE-1-0-G和UKESM1-0-LL两个模式的最长日期没有31日（1月、3月、5月、7月、8月、10月和12月都没有），导致计算有误，不要这两个模式，\n",
    "#bcc-csm2-mr这个模式没有湿度数据，考虑到后面要用到湿度数据，也不要这个模式\n",
    "#这个不能同一台电脑运行多个界面，否则速度将至不到三分之一"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "def is_cdo_available():\n",
    "    \"\"\"\n",
    "    检查CDO是否在WSL中可用。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 使用WSL运行CDO的版本检查命令\n",
    "        subprocess.run([\"wsl\", \"cdo\", \"-V\"], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        return True\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        return False\n",
    "\n",
    "if not is_cdo_available():\n",
    "    print(\"cdo is not installed or not found in PATH in WSL. Please install cdo in WSL and ensure it is in the PATH.\")\n",
    "    exit()\n",
    "\n",
    "# 输入文件夹的Windows路径\n",
    "input_folder = \"L:/GDDP-CMIP6\"\n",
    "\n",
    "# 模型列表\n",
    "models = [\n",
    "    \"ACCESS-CM2\", \"ACCESS-ESM1-5\", \"CanESM5\", \n",
    "    \"CMCC-CM2-SR5\", \"CMCC-ESM2\", \"CNRM-CM6-1\", \"CNRM-ESM2-1\", \n",
    "    \"EC-Earth3\", \"EC-Earth3-Veg-LR\", \"FGOALS-g3\", \"GFDL-ESM4\", \n",
    "    \"GISS-E2-1-G\", \"INM-CM4-8\", \"INM-CM5-0\", \"IPSL-CM6A-LR\", \n",
    "    \"MIROC6\", \"MIROC-ES2L\", \"MPI-ESM1-2-HR\", \"MPI-ESM1-2-LR\", \n",
    "    \"MRI-ESM2-0\", \"NorESM2-LM\", \"NorESM2-MM\", \"TaiESM1\",\n",
    "]\n",
    "\n",
    "# 情景和对应年份范围的字典\n",
    "scenarios_years = {\n",
    "    \"ssp126\": range(2015, 2101),\n",
    "    \"ssp245\": range(2015, 2101),\n",
    "    \"ssp370\": range(2015, 2101),\n",
    "    \"ssp585\": range(2015, 2101)\n",
    "}\n",
    "\n",
    "# 遍历每个情景和其对应的年份范围\n",
    "for scenario, years in scenarios_years.items():\n",
    "    output_folder = f\"L:/CMIP6-1new/{scenario}\"  # 根据情景设置输出文件夹\n",
    "\n",
    "    # 创建输出文件夹（如果不存在）\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for year in tqdm(years):\n",
    "        start_time = datetime.now()  # 记录开始时间\n",
    "\n",
    "        files_to_process = []\n",
    "\n",
    "        # 遍历所有模型\n",
    "        for model in models:\n",
    "            model_path = os.path.join(input_folder, model, scenario, \"tas\")\n",
    "\n",
    "            # 根据模型和情景构建文件名\n",
    "            if model in [\"CNRM-CM6-1\", \"CNRM-ESM2-1\"]:\n",
    "                file_name = f\"tas_day_{model}_{scenario}_r1i1p1f2_gr_{year}.nc\"\n",
    "            elif model in [\"FGOALS-g3\"]:\n",
    "                file_name = f\"tas_day_{model}_{scenario}_r3i1p1f1_gn_{year}.nc\"\n",
    "            elif model in [\"GFDL-ESM4\", \"INM-CM4-8\", \"INM-CM5-0\"]:\n",
    "                file_name = f\"tas_day_{model}_{scenario}_r1i1p1f1_gr1_{year}.nc\"\n",
    "            elif model in [\"GISS-E2-1-G\", \"MIROC-ES2L\"]:\n",
    "                file_name = f\"tas_day_{model}_{scenario}_r1i1p1f2_gn_{year}.nc\"\n",
    "            elif model in [\"EC-Earth3\", \"EC-Earth3-Veg-LR\", \"IPSL-CM6A-LR\"]:\n",
    "                file_name = f\"tas_day_{model}_{scenario}_r1i1p1f1_gr_{year}.nc\"\n",
    "            else:\n",
    "                file_name = f\"tas_day_{model}_{scenario}_r1i1p1f1_gn_{year}.nc\"\n",
    "                \n",
    "            file_path = os.path.join(model_path, file_name)  # 构建完整的文件路径\n",
    "\n",
    "            # 将Windows路径转换为WSL路径\n",
    "            wsl_file_path = file_path.replace(\"L:\", \"/mnt/l\").replace(\"\\\\\", \"/\")\n",
    "\n",
    "            if os.path.exists(file_path):\n",
    "                files_to_process.append(wsl_file_path)\n",
    "            else:\n",
    "                print(f\"Warning: File not found {file_path}\")\n",
    "\n",
    "        # 如果有文件要处理\n",
    "        if files_to_process:\n",
    "            output_file = os.path.join(output_folder, f\"tas_day_{scenario}_r1i1p1f1_gn_{year}.nc\")\n",
    "            wsl_output_file = output_file.replace(\"L:\", \"/mnt/l\").replace(\"\\\\\", \"/\")\n",
    "\n",
    "            cdo_command = [\"wsl\", \"cdo\", \"ensmean\"] + files_to_process + [wsl_output_file]\n",
    "\n",
    "            try:\n",
    "                subprocess.run(cdo_command, check=True)\n",
    "                print(f\"Processed and saved data for scenario {scenario}, year {year}\")\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"Error processing data for scenario {scenario}, year {year}: {e}\")\n",
    "\n",
    "            end_time = datetime.now()  # 记录结束时间\n",
    "            elapsed_time = end_time - start_time\n",
    "            print(f\"Time taken for scenario {scenario}, year {year}: {elapsed_time}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"No data found for scenario {scenario}, year {year}. Skipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#计算26个模式每日均值，用的是CDO，每年大概需要4分钟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "def is_cdo_available():\n",
    "    \"\"\"\n",
    "    检查CDO是否在WSL中可用。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 使用WSL运行CDO的版本检查命令\n",
    "        subprocess.run([\"wsl\", \"cdo\", \"-V\"], check=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "        return True\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        return False\n",
    "\n",
    "if not is_cdo_available():\n",
    "    print(\"cdo is not installed or not found in PATH in WSL. Please install cdo in WSL and ensure it is in the PATH.\")\n",
    "    exit()\n",
    "\n",
    "# 输入文件夹的Windows路径\n",
    "input_folder = \"L:/GDDP-CMIP6\"\n",
    "\n",
    "# 模型列表\n",
    "models = [\n",
    "    \"ACCESS-CM2\", \"ACCESS-ESM1-5\", \"BCC-CSM2-MR\", \"CanESM5\", \n",
    "    \"CMCC-CM2-SR5\", \"CMCC-ESM2\", \"CNRM-CM6-1\", \"CNRM-ESM2-1\", \n",
    "    \"EC-Earth3\", \"EC-Earth3-Veg-LR\", \"FGOALS-g3\", \"GFDL-ESM4\", \n",
    "    \"GISS-E2-1-G\", \"INM-CM4-8\", \"INM-CM5-0\", \"IPSL-CM6A-LR\", \n",
    "    \"KACE-1-0-G\", \"MIROC6\", \"MIROC-ES2L\", \"MPI-ESM1-2-HR\", \n",
    "    \"MPI-ESM1-2-LR\", \"MRI-ESM2-0\", \"NorESM2-LM\", \"NorESM2-MM\", \n",
    "    \"TaiESM1\", \"UKESM1-0-LL\"\n",
    "]\n",
    "\n",
    "# 情景和对应年份范围的字典\n",
    "scenarios_years = {\n",
    "    \"ssp126\": range(2015, 2101),  # 情景ssp126，年份为2032到2033年\n",
    "    \"ssp245\": range(2015, 2101),  # 情景ssp245，年份为2067到2100年\n",
    "    \"ssp370\": range(2015, 2101),  # 情景ssp370，年份为2015到2100年\n",
    "    \"ssp585\": range(2015, 2101)   # 情景ssp585，年份为2015到2100年\n",
    "}\n",
    "\n",
    "# 遍历每个情景和其对应的年份范围\n",
    "for scenario, years in scenarios_years.items():\n",
    "    output_folder = f\"L:/CMIP6-1/{scenario}\"  # 根据情景设置输出文件夹\n",
    "\n",
    "    # 创建输出文件夹（如果不存在）\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for year in tqdm(years):\n",
    "        files_to_process = []\n",
    "\n",
    "        # 遍历所有模型\n",
    "        for model in models:\n",
    "            model_path = os.path.join(input_folder, model, scenario, \"tas\")\n",
    "\n",
    "            # 根据模型和情景构建文件名\n",
    "            if model in [\"CNRM-CM6-1\", \"CNRM-ESM2-1\"]:\n",
    "                file_name = f\"tas_day_{model}_{scenario}_r1i1p1f2_gr_{year}.nc\"\n",
    "            elif model in [\"FGOALS-g3\"]:\n",
    "                file_name = f\"tas_day_{model}_{scenario}_r3i1p1f1_gn_{year}.nc\"\n",
    "            elif model in [\"GFDL-ESM4\", \"INM-CM4-8\", \"INM-CM5-0\"]:\n",
    "                file_name = f\"tas_day_{model}_{scenario}_r1i1p1f1_gr1_{year}.nc\"\n",
    "            elif model in [\"GISS-E2-1-G\", \"MIROC-ES2L\", \"UKESM1-0-LL\"]:\n",
    "                file_name = f\"tas_day_{model}_{scenario}_r1i1p1f2_gn_{year}.nc\"\n",
    "            elif model in [\"EC-Earth3\", \"EC-Earth3-Veg-LR\", \"IPSL-CM6A-LR\", \"KACE-1-0-G\"]:\n",
    "                file_name = f\"tas_day_{model}_{scenario}_r1i1p1f1_gr_{year}.nc\"\n",
    "            else:\n",
    "                file_name = f\"tas_day_{model}_{scenario}_r1i1p1f1_gn_{year}.nc\"\n",
    "\n",
    "            file_path = os.path.join(model_path, file_name)  # 构建完整的文件路径\n",
    "\n",
    "            # 将Windows路径转换为WSL路径\n",
    "            wsl_file_path = file_path.replace(\"L:\", \"/mnt/l\").replace(\"\\\\\", \"/\")\n",
    "\n",
    "            if os.path.exists(file_path):\n",
    "                files_to_process.append(wsl_file_path)\n",
    "            else:\n",
    "                print(f\"Warning: File not found {file_path}\")\n",
    "\n",
    "        # 如果有文件要处理\n",
    "        if files_to_process:\n",
    "            output_file = os.path.join(output_folder, f\"tas_day_{scenario}_r1i1p1f1_gn_{year}.nc\")\n",
    "            wsl_output_file = output_file.replace(\"L:\", \"/mnt/l\").replace(\"\\\\\", \"/\")\n",
    "\n",
    "            cdo_command = [\"wsl\", \"cdo\", \"ensmean\"] + files_to_process + [wsl_output_file]\n",
    "\n",
    "            try:\n",
    "                subprocess.run(cdo_command, check=True)\n",
    "                print(f\"Processed and saved data for scenario {scenario}, year {year}\")\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                print(f\"Error processing data for scenario {scenario}, year {year}: {e}\")\n",
    "        else:\n",
    "            print(f\"No data found for scenario {scenario}, year {year}. Skipping.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#计算完结果后，用下面代码检查下结果是否包含一年所有日期。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "def check_for_december_31st(input_folder, scenarios, historical=False):\n",
    "    \"\"\"\n",
    "    检查特定情景和年份的NetCDF文件是否包含12月31日的数据。\n",
    "    \"\"\"\n",
    "    # 设置年份范围\n",
    "    year_range = range(1990, 2015) if historical else range(2015, 2101)\n",
    "\n",
    "    for scenario in scenarios:\n",
    "        for year in year_range:\n",
    "            # 构建文件路径\n",
    "            if historical:\n",
    "                file_name = f\"tas_day_historical_r1i1p1f1_gn_{year}.nc\"\n",
    "                file_path = os.path.join(input_folder, \"historical\", file_name)\n",
    "            else:\n",
    "                file_name = f\"tas_day_{scenario}_r1i1p1f1_gn_{year}.nc\"\n",
    "                file_path = os.path.join(input_folder, scenario, file_name)\n",
    "            \n",
    "            wsl_file_path = file_path.replace(\"L:\", \"/mnt/l\").replace(\"\\\\\", \"/\")\n",
    "\n",
    "            if os.path.exists(file_path):\n",
    "                try:\n",
    "                    # 使用CDO的showdate命令显示文件中的日期\n",
    "                    result = subprocess.run([\"wsl\", \"cdo\", \"showdate\", wsl_file_path], \n",
    "                                            stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True, check=True)\n",
    "                    dates = result.stdout.strip().split()\n",
    "                    december_31st = f\"{year}-12-31\"\n",
    "                    if december_31st in dates:\n",
    "                        print(f\"December 31st is present in {file_path}\")\n",
    "                    else:\n",
    "                        print(f\"December 31st is missing in {file_path}\")\n",
    "                except subprocess.CalledProcessError as e:\n",
    "                    print(f\"Error processing file {file_path}: {e}\")\n",
    "            else:\n",
    "                print(f\"File not found: {file_path}\")\n",
    "\n",
    "# 输入文件夹的Windows路径\n",
    "input_folder = \"L:/CMIP6-1new\"\n",
    "\n",
    "# 情景列表\n",
    "scenarios = [\"ssp126\", \"ssp245\", \"ssp370\", \"ssp585\"]\n",
    "\n",
    "# 检查2025年的数据\n",
    "check_for_december_31st(input_folder, scenarios)\n",
    "\n",
    "# 检查历史数据\n",
    "check_for_december_31st(input_folder, scenarios, historical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#重要！上述的代码计算了25个模式的均值，如下代码则基于湿度和温度数据计算人体热指数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# 定义情景和年份\n",
    "scenarios = ['ssp126', 'ssp245','ssp370', 'ssp585']\n",
    "years = range(2015, 2101)\n",
    "\n",
    "#scenarios = ['historical']\n",
    "#years = range(1990, 2015)\n",
    "\n",
    "for scenario in scenarios:\n",
    "    for year in years:\n",
    "        # 构建文件路径\n",
    "        temperature_file = f'L:/CMIP6-1final/{scenario}/tas_day_{scenario}_r1i1p1f1_gn_{year}.nc'\n",
    "        humidity_file = f'M:/CMIP6-1final/{scenario}/hurs_day_{scenario}_r1i1p1f1_gn_{year}.nc'\n",
    "        output_file = f'L:/CMIP6-1NEW/{scenario}/hi_day_{scenario}_r1i1p1f1_gn_{year}.nc'\n",
    "\n",
    "        # 检查并创建输出目录\n",
    "        output_directory = os.path.dirname(output_file)\n",
    "        if not os.path.exists(output_directory):\n",
    "            os.makedirs(output_directory)\n",
    "\n",
    "        # 检查输出文件是否存在\n",
    "        if not os.path.exists(output_file):\n",
    "            # 加载数据\n",
    "            temp_data = xr.open_dataset(temperature_file)\n",
    "            humidity_data = xr.open_dataset(humidity_file)\n",
    "\n",
    "            # 获取温度和相对湿度的数据\n",
    "            T = temp_data['tas']  # 'tas' 变量为温度\n",
    "            RH = humidity_data['hurs']  # 'hurs' 变量为湿度\n",
    "\n",
    "            # 计算热指数（HI）\n",
    "            T_celsius = T - 273.15\n",
    "            HI = xr.where((T <= 299.81667) | (RH <= 40), T_celsius, \n",
    "                          -8.78469475556 + 1.61139411 * T_celsius + 2.33854883889 * RH \n",
    "                          - 0.14611605 * T_celsius * RH - 0.012308094 * (T_celsius ** 2) \n",
    "                          - 0.0164248277778 * (RH ** 2) + 0.002211732 * (T_celsius ** 2) * RH \n",
    "                          + 0.00072546 * T_celsius * (RH ** 2) - 0.000003582 * (T_celsius ** 2) * (RH ** 2))\n",
    "\n",
    "            # 保存结果为新的NetCDF文件\n",
    "            HI.to_netcdf(output_file)\n",
    "\n",
    "            # 显示当前时间\n",
    "            print(f\"已完成 {scenario} 情景 {year} 年的数据，当前时间: {datetime.datetime.now()}\")\n",
    "        else:\n",
    "            print(f\"文件 {output_file} 已存在，跳过此文件。\")\n",
    "\n",
    "print(\"所有热指数计算完成。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#对0.25°*0.25°原始数据进行降尺度处理，转变为0.125°*0.125°数据，但需要注意的是新生成数据的extent为top为90.0625，left为-0.0625, right为360.0625, bottom为-60.0625\n",
    "#原始数据的extent 为top为90，left为0，right为360，bottom为-60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 定义年份和情景\n",
    "years = [1990] + list(range(2000, 2020)) + list(range(2020, 2101, 10))\n",
    "scenarios = ['ssp126', 'ssp245', 'ssp370', 'ssp585']\n",
    "\n",
    "# 定义原始数据和重采样数据的文件夹路径\n",
    "input_folder = 'L:/CMIP6-1NEW0.25'\n",
    "output_folder = 'L:/CMIP6-1NEW0.125'\n",
    "\n",
    "# 创建输出文件夹（如果不存在）\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    print(f\"创建文件夹：{output_folder}\")\n",
    "\n",
    "for scenario in scenarios:\n",
    "    scenario_output_folder = os.path.join(output_folder, scenario)\n",
    "    if not os.path.exists(scenario_output_folder):\n",
    "        os.makedirs(scenario_output_folder, exist_ok=True)\n",
    "        print(f\"创建文件夹：{scenario_output_folder}\")\n",
    "\n",
    "    for year in years:\n",
    "        input_file = f'{input_folder}/{scenario}/hi_day_{scenario}_r1i1p1f1_gn_{year}.nc'\n",
    "        output_file = f'{scenario_output_folder}/hi_day_{scenario}_r1i1p1f1_gn_{year}_0.125.nc'\n",
    "\n",
    "        # 确保输入文件存在\n",
    "        if os.path.exists(input_file):\n",
    "            ds = xr.open_dataset(input_file)\n",
    "\n",
    "            # 定义新的纬度和经度，确保边界与原始数据对齐\n",
    "            new_lat = np.arange(-60, 90.125, 0.125)  # 修改范围，包括90度\n",
    "            new_lon = np.arange(0, 360.125, 0.125)   # 修改范围，包括360度\n",
    "\n",
    "            # 确保包含边界\n",
    "            if new_lat[-1] > 90:\n",
    "                new_lat = new_lat[:-1]\n",
    "            if new_lon[-1] > 360:\n",
    "                new_lon = new_lon[:-1]\n",
    "\n",
    "            # 使用最近邻插值进行重采样\n",
    "            ds_resampled = ds.interp(lat=new_lat, lon=new_lon, method='nearest')\n",
    "\n",
    "            # 保留原始数据集的属性\n",
    "            ds_resampled.attrs = ds.attrs\n",
    "            ds_resampled.lat.attrs = ds.lat.attrs\n",
    "            ds_resampled.lon.attrs = ds.lon.attrs\n",
    "\n",
    "            # 保存重采样后的数据集\n",
    "            ds_resampled.to_netcdf(output_file)\n",
    "\n",
    "            print(f'Resampled {scenario} for year {year}')\n",
    "        else:\n",
    "            print(f\"输入文件不存在：{input_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#重要！计算全球气温阈值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import netCDF4 as nc\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "import time  # 引入 time 模块\n",
    "\n",
    "def calculate_threshold(data, threshold=18.3, increment=0.2):\n",
    "    nodata_mask = data.isnull().all(dim='time')\n",
    "    count_above_threshold = (data > threshold).sum(dim='time')\n",
    "    mean_temp = data.where(data > threshold).mean(dim='time')\n",
    "    new_threshold = xr.where(count_above_threshold >= 3, threshold + increment * (mean_temp - threshold), threshold)\n",
    "    new_threshold = xr.where(nodata_mask, np.nan, new_threshold)\n",
    "    return new_threshold\n",
    "\n",
    "def process_data(start_year=2000, end_year=2100, step=5, scenarios=['ssp126']):\n",
    "    for scenario in scenarios:\n",
    "        for year in range(start_year, end_year + 1, step):\n",
    "            process_year_data(year, scenario)\n",
    "            \n",
    "def process_year_data(year, scenario):\n",
    "    start_time = time.time()  # 记录处理开始时间\n",
    "    input_directory = f'L:/CMIP6-1NEW/{scenario}/'\n",
    "    output_directory = f'L:/CMIP6-1yuzhi/{scenario}/'\n",
    "\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    output_file_path = f'{output_directory}hi_yuzhi_day_{scenario}_r1i1p1f1_gn_{year}.nc'\n",
    "    all_dates = pd.date_range(start=f\"{year}-01-01\", end=f\"{year}-12-31\")\n",
    "\n",
    "    # 根据2019年数据的分辨率设置纬度和经度大小\n",
    "    lat_size = 600  # 纬度大小\n",
    "    lon_size = 1440  # 经度大小\n",
    "\n",
    "    # 初始化年度数据集\n",
    "    with nc.Dataset(output_file_path, 'w', format='NETCDF4') as ds:\n",
    "        # 创建时间维度\n",
    "        ds.createDimension('time', None)\n",
    "        time_var = ds.createVariable('time', 'f8', ('time',))\n",
    "        time_var.units = 'hours since 1800-01-01'\n",
    "        time_var.calendar = 'gregorian'\n",
    "        time_var.standard_name = 'time'\n",
    "        time_var.long_name = 'time'\n",
    "        time_var.axis = 'T'\n",
    "\n",
    "        # 创建纬度和经度维度\n",
    "        ds.createDimension('lat', lat_size)\n",
    "        ds.createDimension('lon', lon_size)\n",
    "        lat_var = ds.createVariable('lat', 'f4', ('lat',))\n",
    "        lon_var = ds.createVariable('lon', 'f4', ('lon',))\n",
    "        lat_var[:] = np.linspace(-59.875, 89.875, lat_size)\n",
    "        lon_var[:] = np.linspace(0.125, 359.875, lon_size)\n",
    "\n",
    "        # 设置纬度和经度的坐标系统属性\n",
    "        lat_var.standard_name = 'latitude'\n",
    "        lat_var.long_name = 'latitude'\n",
    "        lat_var.units = 'degrees_north'\n",
    "        lat_var.axis = 'Y'\n",
    "        lat_var.crs = 'EPSG:4326'\n",
    "\n",
    "        lon_var.standard_name = 'longitude'\n",
    "        lon_var.long_name = 'longitude'\n",
    "        lon_var.units = 'degrees_east'\n",
    "        lon_var.axis = 'X'\n",
    "        lon_var.crs = 'EPSG:4326'\n",
    "\n",
    "        # 创建数据变量\n",
    "        data_var = ds.createVariable('__xarray_dataarray_variable__', 'f4', ('time', 'lat', 'lon'), fill_value=np.nan)\n",
    "\n",
    "    # 每日数据处理\n",
    "    for date in all_dates:\n",
    "        month = date.month\n",
    "        day = date.day\n",
    "        past_data = []\n",
    "        for past_year in range(year - 5, year):\n",
    "            file_path = f'{input_directory}hi_day_{scenario}_r1i1p1f1_gn_{past_year}.nc'  # 修改为动态情景名称\n",
    "            if os.path.exists(file_path):\n",
    "                with xr.open_dataset(file_path) as ds:\n",
    "                    past_date_str = date.strftime(f\"{past_year}-%m-%d\")\n",
    "                    if (month, day) == (2, 29) and not calendar.isleap(past_year):\n",
    "                        past_date_str = f\"{past_year}-02-28\"\n",
    "                    ds = ds.sel(time=past_date_str)\n",
    "                    past_data.append(ds)\n",
    "\n",
    "        if past_data:\n",
    "            combined_data = xr.concat(past_data, dim='time')\n",
    "            threshold_data = calculate_threshold(combined_data['__xarray_dataarray_variable__'])\n",
    "            \n",
    "            with nc.Dataset(output_file_path, 'a') as dst:\n",
    "                time_index = len(dst['time'])\n",
    "                # 为了与2019年数据对齐，将时间设置为中午12点\n",
    "                dst['time'][time_index] = nc.date2num(date.replace(hour=12), dst['time'].units, dst['time'].calendar)\n",
    "                dst['__xarray_dataarray_variable__'][time_index, :, :] = threshold_data.values\n",
    "\n",
    "    end_time = time.time()  # 记录处理结束时间\n",
    "    elapsed_time = end_time - start_time  # 计算所用时间\n",
    "    print(f'年度数据集 {output_file_path} 已成功保存。用时: {elapsed_time:.2f} 秒')\n",
    "\n",
    "process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#对0.25°*0.25°原始阈值数据进行降尺度处理，转变为0.125°*0.125°数据，但需要注意的是新生成数据的extent为top为90.0625，left为-0.0625, right为360.0625, bottom为-60.0625\n",
    "#原始数据的extent 为top为90，left为0，right为360，bottom为-60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 定义数据文件名\n",
    "filenames = ['hi_yuzhi_day_r1i1p1f1_gn_2000_canshu0.1.nc', 'hi_yuzhi_day_r1i1p1f1_gn_2000_canshu0.2.nc']\n",
    "\n",
    "# 定义原始数据和重采样数据的文件夹路径\n",
    "input_folder = 'L:/CMIP6-1yuzhi0.25'\n",
    "output_folder = 'L:/CMIP6-1yuzhi0.125'\n",
    "\n",
    "# 创建输出文件夹（如果不存在）\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    print(f\"创建文件夹：{output_folder}\")\n",
    "\n",
    "# 循环处理每个文件\n",
    "for filename in filenames:\n",
    "    input_file = os.path.join(input_folder, filename)\n",
    "    output_file = os.path.join(output_folder, filename.replace('.nc', '_0.125.nc'))\n",
    "\n",
    "    # 确保输入文件存在\n",
    "    if os.path.exists(input_file):\n",
    "        ds = xr.open_dataset(input_file)\n",
    "\n",
    "        # 定义新的纬度和经度，确保边界与原始数据对齐\n",
    "        new_lat = np.arange(-60, 90.125, 0.125)  # 修改范围，包括90度\n",
    "        new_lon = np.arange(0, 360.125, 0.125)   # 修改范围，包括360度\n",
    "\n",
    "        # 确保包含边界\n",
    "        if new_lat[-1] > 90:\n",
    "            new_lat = new_lat[:-1]\n",
    "        if new_lon[-1] > 360:\n",
    "            new_lon = new_lon[:-1]\n",
    "\n",
    "        # 使用最近邻插值进行重采样\n",
    "        ds_resampled = ds.interp(lat=new_lat, lon=new_lon, method='nearest')\n",
    "\n",
    "        # 保留原始数据集的属性\n",
    "        ds_resampled.attrs = ds.attrs\n",
    "        ds_resampled.lat.attrs = ds.lat.attrs\n",
    "        ds_resampled.lon.attrs = ds.lon.attrs\n",
    "\n",
    "        # 保存重采样后的数据集\n",
    "        ds_resampled.to_netcdf(output_file)\n",
    "\n",
    "        print(f'Resampled and saved: {output_file}')\n",
    "    else:\n",
    "        print(f\"输入文件不存在：{input_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#将历史数据另存到各个情景文件夹中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# 定义源目录和目标目录\n",
    "source_directory = 'L:/CMIP6-1NEW/historical/'\n",
    "target_directory = 'L:/CMIP6-1NEW/ssp585/'\n",
    "\n",
    "# 定义需要处理的年份范围\n",
    "years = range(1995, 2015)\n",
    "\n",
    "# 遍历每个年份，复制并重命名文件\n",
    "for year in years:\n",
    "    source_file = f'{source_directory}hi_day_historical_r1i1p1f1_gn_{year}.nc'\n",
    "    target_file = f'{target_directory}hi_day_ssp585_r1i1p1f1_gn_{year}.nc'\n",
    "    \n",
    "    # 检查源文件是否存在\n",
    "    if os.path.exists(source_file):\n",
    "        # 复制并重命名文件\n",
    "        shutil.copyfile(source_file, target_file)\n",
    "        print(f'文件 {year} 已复制和重命名。')\n",
    "    else:\n",
    "        print(f'源文件 {year} 不存在，跳过。')\n",
    "\n",
    "print('所有文件处理完成。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#重点！合并日度数据为年度数据时一定要用netCDF4库！用其他的都容易出错\n",
    "#考虑到“温度越高的地方，居民往往越耐热”。基于此，我们基于历史前五年数据计算该年的温度阈值，\n",
    "#以栅格a为例，其在1月1日温度阈值假设为Ta0101。如果该栅格在2020年前五年（2015-2019年）的1月1日中没有3个以上年份的温度高于20摄氏度，那2020年1月1日温度阈值Ta0101为20摄氏度；如果栅格a在2020年前五年（2015-2019年）1月1日中存在3个以上年份的温度高于20摄氏度，那先计算栅格a这五年中高于20摄氏度的温度的均值Ta0101mean，而该栅格a在2020年1月1日的温度阈值Ta0101则为20+0.1*（Ta0101mean-20）。其他栅格的计算依次类推。直至计算出该年所有栅格的温度阈值。\n",
    "#闰年（比如2020年）是有2月29日，但其前面五年（比如2015-2019年）中很多年份都没有2月29日，为此，闰年（比如2020年）所有栅格2月29日的温度阈值直接等同于该年对应栅格2月28日的温度阈值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import netCDF4 as nc\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "\n",
    "def calculate_threshold(data, threshold=20, increment=0.1):\n",
    "    nodata_mask = data.isnull().all(dim='time')\n",
    "    count_above_threshold = (data > threshold).sum(dim='time')\n",
    "    mean_temp = data.where(data > threshold).mean(dim='time')\n",
    "    new_threshold = xr.where(count_above_threshold >= 3, threshold + increment * (mean_temp - threshold), threshold)\n",
    "    new_threshold = xr.where(nodata_mask, np.nan, new_threshold)\n",
    "    return new_threshold\n",
    "\n",
    "\n",
    "def process_2020_data():\n",
    "    input_directory = 'L:/CMIP6-1NEW/ssp126/'\n",
    "    output_directory = 'L:/CMIP6-1yuzhi12bf/ssp126/'\n",
    "    year = 2020\n",
    "\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    output_file_path = f'{output_directory}hi_yuzhi_day_ssp126_r1i1p1f1_gn_{year}.nc'\n",
    "    all_dates = pd.date_range(start=f\"{year}-01-01\", end=f\"{year}-12-31\")\n",
    "\n",
    "    # 根据2019年数据的分辨率设置纬度和经度大小\n",
    "    lat_size = 600  # 纬度大小\n",
    "    lon_size = 1440  # 经度大小\n",
    "\n",
    "    # 初始化年度数据集\n",
    "    with nc.Dataset(output_file_path, 'w', format='NETCDF4') as ds:\n",
    "        # 创建时间维度\n",
    "        ds.createDimension('time', None)\n",
    "        time_var = ds.createVariable('time', 'f8', ('time',))\n",
    "        time_var.units = 'hours since 1800-01-01'\n",
    "        time_var.calendar = 'gregorian'\n",
    "        time_var.standard_name = 'time'\n",
    "        time_var.long_name = 'time'\n",
    "        time_var.axis = 'T'\n",
    "\n",
    "        # 创建纬度和经度维度\n",
    "        ds.createDimension('lat', lat_size)\n",
    "        ds.createDimension('lon', lon_size)\n",
    "        lat_var = ds.createVariable('lat', 'f4', ('lat',))\n",
    "        lon_var = ds.createVariable('lon', 'f4', ('lon',))\n",
    "        lat_var[:] = np.linspace(-59.875, 89.875, lat_size)  # 根据2019年数据调整\n",
    "        lon_var[:] = np.linspace(0.125, 359.875, lon_size)  # 根据2019年数据调整\n",
    "\n",
    "        # 创建数据变量\n",
    "        data_var = ds.createVariable('__xarray_dataarray_variable__', 'f4', ('time', 'lat', 'lon'), fill_value=np.nan)\n",
    "\n",
    "    # 每日数据处理\n",
    "    for date in all_dates:\n",
    "        month = date.month\n",
    "        day = date.day\n",
    "        past_data = []\n",
    "        for past_year in range(year - 5, year):\n",
    "            file_path = f'{input_directory}hi_day_ssp126_r1i1p1f1_gn_{past_year}.nc'\n",
    "            if os.path.exists(file_path):\n",
    "                with xr.open_dataset(file_path) as ds:\n",
    "                    past_date_str = date.strftime(f\"{past_year}-%m-%d\")\n",
    "                    if (month, day) == (2, 29) and not calendar.isleap(past_year):\n",
    "                        past_date_str = f\"{past_year}-02-28\"\n",
    "                    ds = ds.sel(time=past_date_str)\n",
    "                    past_data.append(ds)\n",
    "\n",
    "        if past_data:\n",
    "            combined_data = xr.concat(past_data, dim='time')\n",
    "            threshold_data = calculate_threshold(combined_data['__xarray_dataarray_variable__'])\n",
    "            \n",
    "            with nc.Dataset(output_file_path, 'a') as dst:\n",
    "                time_index = len(dst['time'])\n",
    "                # 为了与2019年数据对齐，将时间设置为中午12点\n",
    "                dst['time'][time_index] = nc.date2num(date.replace(hour=12), dst['time'].units, dst['time'].calendar)\n",
    "                dst['__xarray_dataarray_variable__'][time_index, :, :] = threshold_data.values\n",
    "\n",
    "    print(f'年度数据集 {output_file_path} 已成功保存。')\n",
    "\n",
    "process_2020_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1、计算每日的气温阈值数据都需要读取1950-1999年的750天的气温数据，是15天乘以50年。采用的方法是读取1950~1999年期间同一天的15天窗口。比如计算2000年1月1日的气温阈值需要读取的是1950-1999年1月1日至1月15日，计算1月2日的气温阈值需要读取的是1950-1999年1月1日至1月15日，计算1月3日的气温阈值需要读取的是1950-1999年1月1日至1月15日，依次类推，而计算1月8日的气温阈值需要读取的是1950-1999年1月1日至1月15日，计算1月9日的气温阈值需要读取的是1950-1999年1月2日至1月16日，计算1月10日的气温阈值需要读取的是1950-1999年1月3日至1月17日，依次类推，计算1月26日的气温阈值需要读取的是1950-1999年1月19日至2月2日，计算1月27日的气温阈值需要读取的是1950-1999年1月20日至2月3日，依次类推，计算2月21日的气温阈值需要读取的是1950-1999年2月14日至2月28日，计算2月22日的气温阈值需要读取的是1950-1999年2月15日至3月1日（闰年2月29日不考虑在内），计算2月23日的气温阈值需要读取的是1950-1999年2月16日至3月2日（闰年2月29日不考虑在内），依次类推，计算2月28日的气温阈值需要读取的是1950-1999年2月21日至3月7日（闰年2月29日不考虑在内），而计算2月29日的气温阈值时则直接等于前面计算出的2月28日的气温阈值，计算3月1日的气温阈值需要读取的是1950-1999年2月22日至3月8日（闰年2月29日不考虑在内），计算3月2日的气温阈值需要读取的是1950-1999年2月23日至3月9日（闰年2月29日不考虑在内），依次类推，计算3月8日的气温阈值需要读取的是1950-1999年3月1日至3月15日，依次类推，计算12月29日的气温阈值需要读取的是1950-1999年12月17日至12月31日，计算12月30日的气温阈值需要读取的是1950-1999年12月17日至12月31日，计算12月28日的气温阈值需要读取的是1950-1999年12月17日至12月31日。\n",
    "#2、以栅格a为例，其在2000年1月1日温度阈值假设为Ta0101。如果该栅格在该日1950-1999年对应的150天中没有60%的天数的温度高于18.3，那该栅格2000年1月1日温度阈值Ta0101为18.3摄氏度；如果该栅格在该日1950-1999年对应的150天中有60%的天数的温度高于18.3，那先计算栅格a这150天中高于18.3摄氏度的温度的均值Ta0101mean，而该栅格a在2000年1月1日的温度阈值Ta0101则为18.3+0.1*（Ta0101mean-18.3）。2000年是闰年，是有2月29日的，该日温度阈值直接用2月28日的。依次计算出栅格b阈值数据的情况，直至计算出所有栅格的阈值数据。\n",
    "#3、生成的数据文件为L:\\CMIP6-1yuzhi\\hi_yuzhi_day_r1i1p1f1_gn_2000.nc, 空间精度为0.25度*0.25度，日期是1月1日-12月31日，日期格式类似于为2000/01/01 12:00:00 AM，2000/01/02 12:00:00 AM，2000/01/03 12:00:00 AM直至2000/12/31 12:00:00 AM。该数据文件的raster information 中columns and rows 是1440,600。number of bands 是1。cell size是0.25°*0.25°。Format是NetCDF。Source type是generic。Pixel type是floating point。pixel Depth是32 Bit。NoData Value 是nan。Status是Permanent。Extent的top是90，bottom是-60，left是0，right是360。XY Coordinate System是GCS_WGS_1984。Angular Unit是Degree（0.0174532925199433）。Datum是D_WGS_1984。\n",
    "#4、当读取1950-1999年栅格a某个日期的数据为nodata，最后生成的阈值数据L:\\CMIP6-1yuzhi\\hi_yuzhi_day _r1i1p1f1_gn_2000.nc该栅格全年也应该是nodata，且不显示。参考这段代码处理方式nodata_mask = data.isnull().all(dim='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "import netCDF4 as nc\n",
    "import pandas as pd\n",
    "import calendar\n",
    "import time\n",
    "\n",
    "def calculate_threshold(data, threshold=18.3, percentage=0.6, increment=0.1):\n",
    "    # 检查每个栅格在整个时间序列中是否全部为nodata\n",
    "    nodata_mask = data.isnull().all(dim='time')\n",
    "\n",
    "    # 只在非nodata的数据上计算阈值\n",
    "    valid_data_mask = ~nodata_mask\n",
    "\n",
    "    count_above_threshold = (data > threshold).where(valid_data_mask).sum(dim='time')\n",
    "    mean_temp_above_threshold = data.where(data > threshold).where(valid_data_mask).mean(dim='time')\n",
    "\n",
    "    percentage_above_threshold = count_above_threshold / len(data.time)\n",
    "    adjusted_threshold = threshold + increment * (mean_temp_above_threshold - threshold)\n",
    "    new_threshold = xr.where(percentage_above_threshold >= percentage, adjusted_threshold, threshold)\n",
    "    \n",
    "    new_threshold = xr.where(nodata_mask, np.nan, new_threshold)  # 保留nodata的位置\n",
    "\n",
    "    return new_threshold\n",
    "\n",
    "def get_date_ranges(date):\n",
    "    year = date.year\n",
    "    month = date.month\n",
    "    day = date.day\n",
    "\n",
    "    # 确定每个月的天数\n",
    "    _, num_days_in_month = calendar.monthrange(year, month)\n",
    "\n",
    "    # 计算15天窗口的开始和结束日期\n",
    "    if month == 1 and day < 8:\n",
    "        start_day = 1\n",
    "        end_day = 15\n",
    "    elif month == 12 and day > 22:\n",
    "        start_day = 18\n",
    "        end_day = 31\n",
    "    else:\n",
    "        start_day = max(1, day - 7)\n",
    "        end_day = min(num_days_in_month, day + 7)\n",
    "\n",
    "    # 如果是2月并且天数超过了28，对于非闰年，调整天数\n",
    "    if month == 2 and end_day > 28 and not calendar.isleap(year):\n",
    "        end_day = 28\n",
    "\n",
    "    # 生成开始和结束日期\n",
    "    start_date = pd.Timestamp(year=year, month=month, day=start_day, hour=12)\n",
    "    end_date = pd.Timestamp(year=year, month=month, day=end_day, hour=12)\n",
    "\n",
    "    return start_date.isoformat(), end_date.isoformat()\n",
    "\n",
    "def process_2000_data():\n",
    "    input_directory = 'L:/CMIP6-1NEW/historical/'\n",
    "    output_directory = 'L:/CMIP6-1yuzhi/'\n",
    "    year = 2000\n",
    "\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    output_file_path = f'{output_directory}hi_yuzhi_day_ssp126_r1i1p1f1_gn_{year}.nc'\n",
    "    all_dates = pd.date_range(start=f\"{year}-01-01\", end=f\"{year}-12-31\")\n",
    "\n",
    "    # 根据2019年数据的分辨率设置纬度和经度大小\n",
    "    lat_size = 600  # 纬度大小\n",
    "    lon_size = 1440  # 经度大小\n",
    "\n",
    "    # 初始化年度数据集\n",
    "    with nc.Dataset(output_file_path, 'w', format='NETCDF4') as ds:\n",
    "        # 创建时间维度\n",
    "        ds.createDimension('time', None)\n",
    "        time_var = ds.createVariable('time', 'f8', ('time',))\n",
    "        time_var.units = 'hours since 1800-01-01'\n",
    "        time_var.calendar = 'gregorian'\n",
    "        time_var.standard_name = 'time'\n",
    "        time_var.long_name = 'time'\n",
    "        time_var.axis = 'T'\n",
    "\n",
    "        # 创建纬度和经度维度\n",
    "        ds.createDimension('lat', lat_size)\n",
    "        ds.createDimension('lon', lon_size)\n",
    "        lat_var = ds.createVariable('lat', 'f4', ('lat',))\n",
    "        lon_var = ds.createVariable('lon', 'f4', ('lon',))\n",
    "        lat_var[:] = np.linspace(-59.875, 89.875, lat_size)  # 根据2019年数据调整\n",
    "        lon_var[:] = np.linspace(0.125, 359.875, lon_size)  # 根据2019年数据调整\n",
    "        lat_var.standard_name = 'latitude'\n",
    "        lat_var.long_name = 'latitude'\n",
    "        lat_var.units = 'degrees_north'\n",
    "        lon_var.standard_name = 'longitude'\n",
    "        lon_var.long_name = 'longitude'\n",
    "        lon_var.units = 'degrees_east'\n",
    "\n",
    "        # 创建数据变量\n",
    "        data_var = ds.createVariable('__xarray_dataarray_variable__', 'f4', ('time', 'lat', 'lon'), fill_value=np.nan)\n",
    "\n",
    "        # 添加描述性的全局属性\n",
    "        ds.title = 'Climate Data'\n",
    "        ds.institution = 'Your Institution'\n",
    "        ds.source = 'Climate Model'\n",
    "        ds.references = 'Your References'\n",
    "        ds.comment = 'Processed using netCDF4 Python module'\n",
    "        ds.geospatial_lat_min = -59.875\n",
    "        ds.geospatial_lat_max = 89.875\n",
    "        ds.geospatial_lon_min = 0.125\n",
    "        ds.geospatial_lon_max = 359.875\n",
    "        ds.geospatial_lat_units = \"degrees_north\"\n",
    "        ds.geospatial_lon_units = \"degrees_east\"\n",
    "        \n",
    "       \n",
    "    # 储存上一天的阈值数据，初始为None\n",
    "    previous_day_threshold = None\n",
    "    \n",
    "    for date in all_dates:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # 特别处理闰年的2月29日\n",
    "        if date.month == 2 and date.day == 29:\n",
    "            threshold_data = previous_day_threshold\n",
    "        else:\n",
    "            past_data = []\n",
    "            for past_year in range(1950, 2000):\n",
    "                file_path = f'{input_directory}hi_day_historical_r1i1p1f1_gn_{past_year}.nc'\n",
    "                if os.path.exists(file_path):\n",
    "                    with xr.open_dataset(file_path) as ds:\n",
    "                        start_date, end_date = get_date_ranges(pd.Timestamp(year=past_year, month=date.month, day=date.day, hour=12))\n",
    "                        ds_range = ds.sel(time=slice(start_date, end_date))\n",
    "                        if ds_range['__xarray_dataarray_variable__'].size > 0:\n",
    "                            past_data.append(ds_range['__xarray_dataarray_variable__'])\n",
    "\n",
    "            if past_data:\n",
    "                combined_data = xr.concat(past_data, dim='time')\n",
    "                threshold_data = calculate_threshold(combined_data)\n",
    "\n",
    "            # 存储当前日期的阈值数据以用于下一次循环（如2月29日）\n",
    "            previous_day_threshold = threshold_data\n",
    "\n",
    "            with nc.Dataset(output_file_path, 'a') as dst:\n",
    "                time_index = len(dst['time'])\n",
    "                dst['time'][time_index] = nc.date2num(date.replace(hour=12), dst['time'].units, dst['time'].calendar)\n",
    "                dst['__xarray_dataarray_variable__'][time_index, :, :] = threshold_data.values\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f\"Completed date: {date}, Time taken: {end_time - start_time} seconds\")\n",
    "\n",
    "    print(f'年度数据集 {output_file_path} 已成功保存。')\n",
    "\n",
    "process_2000_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1、\t以栅格a为例，其在1月温度阈值假设为Ta01。如果该栅格在1950-1999年的所有1月中没有60%的日期的温度高于18.3摄氏度，那该栅格a在1月温度阈值Ta01为18.3摄氏度；如果栅格a在1950-1999年1月中存在60%以上日期的温度高于18.3摄氏度，那先计算这些高于18.3摄氏度的日期的气温均值Tmean，而该栅格a在1月的气温阈值Ta01则为18.3+0.2*（Tmean-18.3）。其他栅格的计算依次类推。需要注意的是1月需要读取的日期是1950年-1999年的1月所有日期，为50*31=1550天。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import netCDF4 as nc\n",
    "from datetime import datetime, timedelta\n",
    "import calendar\n",
    "\n",
    "def calculate_threshold(data, threshold=20, increment=0.1):\n",
    "    nodata_mask = data.isnull().all(dim='time')\n",
    "    count_above_threshold = (data > threshold).sum(dim='time')\n",
    "    mean_temp = data.where(data > threshold).mean(dim='time')\n",
    "    new_threshold = xr.where(count_above_threshold >= 3, threshold + increment * (mean_temp - threshold), threshold)\n",
    "    new_threshold = xr.where(nodata_mask, np.nan, new_threshold)\n",
    "    return new_threshold\n",
    "\n",
    "\n",
    "def process_2020_data():\n",
    "    input_directory = 'L:/CMIP6-1NEW/ssp126/'\n",
    "    output_directory = 'L:/CMIP6-1yuzhi12bf/ssp126/'\n",
    "    year = 2020\n",
    "\n",
    "    if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    output_file_path = f'{output_directory}hi_yuzhi_day_ssp126_r1i1p1f1_gn_{year}.nc'\n",
    "    all_dates = pd.date_range(start=f\"{year}-01-01\", end=f\"{year}-12-31\")\n",
    "\n",
    "    # 根据2019年数据的分辨率设置纬度和经度大小\n",
    "    lat_size = 600  # 纬度大小\n",
    "    lon_size = 1440  # 经度大小\n",
    "\n",
    "    # 初始化年度数据集\n",
    "    with nc.Dataset(output_file_path, 'w', format='NETCDF4') as ds:\n",
    "        # 创建时间维度\n",
    "        ds.createDimension('time', None)\n",
    "        time_var = ds.createVariable('time', 'f8', ('time',))\n",
    "        time_var.units = 'hours since 1800-01-01'\n",
    "        time_var.calendar = 'gregorian'\n",
    "        time_var.standard_name = 'time'\n",
    "        time_var.long_name = 'time'\n",
    "        time_var.axis = 'T'\n",
    "\n",
    "        # 创建纬度和经度维度\n",
    "        ds.createDimension('lat', lat_size)\n",
    "        ds.createDimension('lon', lon_size)\n",
    "        lat_var = ds.createVariable('lat', 'f4', ('lat',))\n",
    "        lon_var = ds.createVariable('lon', 'f4', ('lon',))\n",
    "        lat_var[:] = np.linspace(-59.875, 89.875, lat_size)  # 根据2019年数据调整\n",
    "        lon_var[:] = np.linspace(0.125, 359.875, lon_size)  # 根据2019年数据调整\n",
    "\n",
    "        # 创建数据变量\n",
    "        data_var = ds.createVariable('__xarray_dataarray_variable__', 'f4', ('time', 'lat', 'lon'), fill_value=np.nan)\n",
    "\n",
    "    # 每日数据处理\n",
    "    for date in all_dates:\n",
    "        month = date.month\n",
    "        day = date.day\n",
    "        past_data = []\n",
    "        for past_year in range(year - 5, year):\n",
    "            file_path = f'{input_directory}hi_day_ssp126_r1i1p1f1_gn_{past_year}.nc'\n",
    "            if os.path.exists(file_path):\n",
    "                with xr.open_dataset(file_path) as ds:\n",
    "                    past_date_str = date.strftime(f\"{past_year}-%m-%d\")\n",
    "                    if (month, day) == (2, 29) and not calendar.isleap(past_year):\n",
    "                        past_date_str = f\"{past_year}-02-28\"\n",
    "                    ds = ds.sel(time=past_date_str)\n",
    "                    past_data.append(ds)\n",
    "\n",
    "        if past_data:\n",
    "            combined_data = xr.concat(past_data, dim='time')\n",
    "            threshold_data = calculate_threshold(combined_data['__xarray_dataarray_variable__'])\n",
    "            \n",
    "            with nc.Dataset(output_file_path, 'a') as dst:\n",
    "                time_index = len(dst['time'])\n",
    "                # 为了与2019年数据对齐，将时间设置为中午12点\n",
    "                dst['time'][time_index] = nc.date2num(date.replace(hour=12), dst['time'].units, dst['time'].calendar)\n",
    "                dst['__xarray_dataarray_variable__'][time_index, :, :] = threshold_data.values\n",
    "\n",
    "    print(f'年度数据集 {output_file_path} 已成功保存。')\n",
    "\n",
    "process_2020_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#将L:\\CMIP6-1NEW\\historical\\hi_day_historical_r1i1p1f1_gn_1990.nc保存为L:\\CMIP6-1NEW\\ssp126\\hi_day_ssp126_r1i1p1f1_gn_1990.nc\n",
    "#年份包括1990-1995年，情景包括ssp126，ssp245，ssp370，ssp585"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def copy_and_rename_files(start_year=1990, end_year=1995, scenarios=['ssp126', 'ssp245', 'ssp370', 'ssp585']):\n",
    "    source_directory = 'L:/CMIP6-1NEW/historical/'\n",
    "    target_base_directory = 'L:/CMIP6-1NEW/'\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        source_file = f'{source_directory}hi_day_historical_r1i1p1f1_gn_{year}.nc'\n",
    "        \n",
    "        for scenario in scenarios:\n",
    "            target_directory = f'{target_base_directory}{scenario}/'\n",
    "            target_file = f'{target_directory}hi_day_{scenario}_r1i1p1f1_gn_{year}.nc'\n",
    "\n",
    "            # 创建目标文件夹如果它不存在\n",
    "            os.makedirs(target_directory, exist_ok=True)\n",
    "\n",
    "            # 复制和重命名文件\n",
    "            shutil.copy2(source_file, target_file)\n",
    "            print(f'File copied: {source_file} to {target_file}')\n",
    "\n",
    "copy_and_rename_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#对2010-2100年进行重采样处理，原来数据的空间精度为0.125度*0.125度，重采样后的数据精度为0.25度*0.25度\n",
    "#考虑到上述重采样得到的是均值，但是我们需要的是求和，所以我们采用先计算人口密度，再重采样，再求人口数量的方法\n",
    "#这步是计算人口密度的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def calculate_population_density(input_file, output_file):\n",
    "    # 检查输出文件路径是否存在，如果不存在则创建\n",
    "    output_dir = os.path.dirname(output_file)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    with rasterio.open(input_file) as dataset:\n",
    "        # 获取原始数据的 nodata 值\n",
    "        nodata = dataset.nodata\n",
    "\n",
    "        # 读取原始人口数据\n",
    "        population_data = dataset.read(1)\n",
    "\n",
    "        # 创建一个与人口数据相同形状的数组，用于存储每个像素的面积\n",
    "        pixel_area = np.zeros(population_data.shape)\n",
    "\n",
    "        # 计算每个像素的面积（单位：平方公里）\n",
    "        for i in range(pixel_area.shape[0]):\n",
    "            # 获取该行像素的平均纬度\n",
    "            lat = dataset.xy(i, 0)[1]\n",
    "            # 每个像素的高度（公里）\n",
    "            pixel_height_km = 111.32 * 0.125\n",
    "            # 每个像素的宽度（公里）\n",
    "            pixel_width_km = 111.32 * np.cos(np.deg2rad(lat)) * 0.125\n",
    "            # 计算面积\n",
    "            pixel_area[i, :] = pixel_height_km * pixel_width_km\n",
    "\n",
    "        # 计算人口密度：人口数 / 像素面积\n",
    "        population_density = np.where(pixel_area > 0, population_data / pixel_area, nodata)\n",
    "\n",
    "        # 确保原始数据中标记为Nodata的像素在结果中同样标记为Nodata\n",
    "        population_density[population_data == nodata] = nodata\n",
    "\n",
    "        # 将人口密度数据类型转换为 float32\n",
    "        population_density = population_density.astype('float32')\n",
    "\n",
    "        # 保存人口密度数据\n",
    "        with rasterio.open(output_file, 'w', driver='GTiff', height=dataset.height,\n",
    "                           width=dataset.width, count=1,\n",
    "                           dtype='float32', crs=dataset.crs, transform=dataset.transform, nodata=nodata) as dst:\n",
    "            dst.write(population_density, 1)\n",
    "\n",
    "    print(f\"Population density file saved: {output_file}\")\n",
    "\n",
    "# 新的处理逻辑\n",
    "years = range(2010, 2101, 10)\n",
    "ssps = ['ssp1', 'ssp2', 'ssp3', 'ssp5']\n",
    "input_base_path = \"L:/pop/popdynamics-1-8th-pop-base-year-projection-ssp-2000-2100-rev01-proj-\"\n",
    "output_base_path = \"L:/popden/popdynamics-1-8th-pop-base-year-projection-ssp-2000-2100-rev01-proj-\"\n",
    "\n",
    "# 对每个SSP和年份的数据计算人口密度\n",
    "for ssp in ssps:\n",
    "    for year in years:\n",
    "        input_file = f\"{input_base_path}{ssp}-geotiff/{ssp}/Total/GeoTIFF/{ssp}_{year}.tif\"\n",
    "        output_file = f\"{output_base_path}{ssp}-geotiff/{ssp}/Total/GeoTIFF/{ssp}_{year}den.tif\"\n",
    "        calculate_population_density(input_file, output_file)\n",
    "        print(f\"Population density calculated for {ssp}, year: {year}\")\n",
    "\n",
    "print(\"All population density calculations are completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#计算历史数据1990年人口密度，原数据精度为0.008333度*0.008333度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def calculate_population_density_single(input_file, output_file):\n",
    "    # 检查输出文件路径是否存在，如果不存在则创建\n",
    "    output_dir = os.path.dirname(output_file)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    with rasterio.open(input_file) as dataset:\n",
    "        # 获取原始数据的 nodata 值\n",
    "        nodata = dataset.nodata\n",
    "\n",
    "        # 读取原始人口数据\n",
    "        population_data = dataset.read(1)\n",
    "\n",
    "        # 创建一个与人口数据相同形状的数组，用于存储每个像素的面积\n",
    "        pixel_area = np.zeros(population_data.shape)\n",
    "\n",
    "        # 计算每个像素的面积（单位：平方公里）\n",
    "        for i in range(pixel_area.shape[0]):\n",
    "            # 获取该行像素的平均纬度\n",
    "            lat = dataset.xy(i, 0)[1]\n",
    "            # 每个像素的高度（公里）\n",
    "            pixel_height_km = 111.32 * 0.008333\n",
    "            # 每个像素的宽度（公里）\n",
    "            pixel_width_km = 111.32 * np.cos(np.deg2rad(lat)) * 0.008333\n",
    "            # 计算面积\n",
    "            pixel_area[i, :] = pixel_height_km * pixel_width_km\n",
    "\n",
    "        # 计算人口密度：人口数 / 像素面积\n",
    "        population_density = np.where(pixel_area > 0, population_data / pixel_area, nodata)\n",
    "\n",
    "        # 确保原始数据中标记为Nodata的像素在结果中同样标记为Nodata\n",
    "        population_density[population_data == nodata] = nodata\n",
    "\n",
    "        # 将人口密度数据类型转换为 float32\n",
    "        population_density = population_density.astype('float32')\n",
    "\n",
    "        # 保存人口密度数据\n",
    "        with rasterio.open(output_file, 'w', driver='GTiff', height=dataset.height,\n",
    "                           width=dataset.width, count=1,\n",
    "                           dtype='float32', crs=dataset.crs, transform=dataset.transform, nodata=nodata) as dst:\n",
    "            dst.write(population_density, 1)\n",
    "\n",
    "    print(f\"Population density file saved: {output_file}\")\n",
    "\n",
    "# 设定输入和输出文件路径\n",
    "input_file_path = \"L:/pop/popdynamics-global-pop-count-time-series-estimates-1990-geotiff/popdynamics-global-pop-count-time-series-estimates_1990.tif\"\n",
    "output_file_path = \"L:/popden/popdynamics-global-pop-count-time-series-estimates-1990-geotiff/popdynamics-global-pop-count-time-series-estimates_1990den.tif\"\n",
    "\n",
    "# 计算人口密度\n",
    "calculate_population_density_single(input_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#对未来人口密度数据进行重采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.transform import from_origin\n",
    "import os\n",
    "\n",
    "def resample_tif(input_file, output_file, desired_cell_size):\n",
    "    # 检查输出文件路径是否存在，如果不存在则创建\n",
    "    output_dir = os.path.dirname(output_file)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    with rasterio.open(input_file) as dataset:\n",
    "        # 获取原始数据的 nodata 值\n",
    "        nodata = dataset.nodata\n",
    "\n",
    "        # 计算新的宽度和高度\n",
    "        new_width = int(dataset.width * (dataset.res[0] / desired_cell_size))\n",
    "        new_height = int(dataset.height * (dataset.res[1] / desired_cell_size))\n",
    "\n",
    "        # 调整分辨率\n",
    "        data = dataset.read(\n",
    "            out_shape=(\n",
    "                dataset.count,\n",
    "                new_height,\n",
    "                new_width\n",
    "            ),\n",
    "            resampling=Resampling.nearest  # 使用最近邻插值\n",
    "        )\n",
    "\n",
    "        # 创建新的变换参数，确保每个像素的尺寸为0.25度*0.25度\n",
    "        new_transform = from_origin(dataset.bounds.left, dataset.bounds.top, desired_cell_size, desired_cell_size)\n",
    "\n",
    "        # 保存重采样后的文件\n",
    "        with rasterio.open(output_file, 'w', driver='GTiff', height=new_height,\n",
    "                           width=new_width, count=dataset.count,\n",
    "                           dtype=data.dtype, crs=dataset.crs, transform=new_transform, nodata=nodata) as dst:\n",
    "            dst.write(data)\n",
    "\n",
    "    print(f\"File saved: {output_file}\")\n",
    "\n",
    "# 新的处理逻辑\n",
    "desired_cell_size = 0.25  # 欲达到的每个单元格的尺寸\n",
    "years = range(2010, 2101, 10)\n",
    "ssps = ['ssp1','ssp2', 'ssp3', 'ssp5']\n",
    "input_base_path = \"L:/popden/popdynamics-1-8th-pop-base-year-projection-ssp-2000-2100-rev01-proj-\"\n",
    "output_base_path = \"L:/popdenRES/popdynamics-1-8th-pop-base-year-projection-ssp-2000-2100-rev01-proj-\"\n",
    "\n",
    "# 对每个SSP和年份的数据进行重采样\n",
    "for ssp in ssps:\n",
    "    for year in years:\n",
    "        input_file = f\"{input_base_path}{ssp}-geotiff/{ssp}/Total/GeoTIFF/{ssp}_{year}den.tif\"\n",
    "        output_file = f\"{output_base_path}{ssp}-geotiff/{ssp}/Total/GeoTIFF/{ssp}_{year}denRES.tif\"\n",
    "        resample_tif(input_file, output_file, desired_cell_size)\n",
    "        print(f\"Resampling completed for {ssp}, year: {year}\")\n",
    "\n",
    "print(\"All resampling processes are completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#对历史数据1990年人口密度进行重采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.transform import from_origin\n",
    "import os\n",
    "\n",
    "def resample_tif(input_file, output_file, desired_cell_size):\n",
    "    # 检查输出文件路径是否存在，如果不存在则创建\n",
    "    output_dir = os.path.dirname(output_file)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    with rasterio.open(input_file) as dataset:\n",
    "        # 获取原始数据的 nodata 值\n",
    "        nodata = dataset.nodata\n",
    "\n",
    "        # 计算新的宽度和高度\n",
    "        new_width = int(dataset.width * (dataset.res[0] / desired_cell_size))\n",
    "        new_height = int(dataset.height * (dataset.res[1] / desired_cell_size))\n",
    "\n",
    "        # 调整分辨率\n",
    "        data = dataset.read(\n",
    "            out_shape=(\n",
    "                dataset.count,\n",
    "                new_height,\n",
    "                new_width\n",
    "            ),\n",
    "            resampling=Resampling.nearest  # 使用最近邻插值\n",
    "        )\n",
    "\n",
    "        # 创建新的变换参数，确保每个像素的尺寸为0.25度*0.25度\n",
    "        new_transform = from_origin(dataset.bounds.left, dataset.bounds.top, desired_cell_size, desired_cell_size)\n",
    "\n",
    "        # 保存重采样后的文件\n",
    "        with rasterio.open(output_file, 'w', driver='GTiff', height=new_height,\n",
    "                           width=new_width, count=dataset.count,\n",
    "                           dtype=data.dtype, crs=dataset.crs, transform=new_transform, nodata=nodata) as dst:\n",
    "            dst.write(data)\n",
    "\n",
    "    print(f\"File saved: {output_file}\")\n",
    "\n",
    "# 指定文件进行重采样\n",
    "input_file = \"L:/popden/popdynamics-global-pop-count-time-series-estimates-1990-geotiff/popdynamics-global-pop-count-time-series-estimates_1990den.tif\"\n",
    "output_file = \"L:/popdenRES/popdynamics-global-pop-count-time-series-estimates-1990-geotiff/popdynamics-global-pop-count-time-series-estimates_1990denRES.tif\"\n",
    "desired_cell_size = 0.25  # 欲达到的每个单元格的尺寸\n",
    "\n",
    "# 执行重采样\n",
    "resample_tif(input_file, output_file, desired_cell_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#将人口密度数据转变为人口数量数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def calculate_population_count(input_file, output_file):\n",
    "    # 检查输出文件路径是否存在，如果不存在则创建\n",
    "    output_dir = os.path.dirname(output_file)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    with rasterio.open(input_file) as dataset:\n",
    "        # 获取原始数据的 nodata 值\n",
    "        nodata = dataset.nodata\n",
    "\n",
    "        # 读取原始人口密度数据\n",
    "        population_density = dataset.read(1)\n",
    "\n",
    "        # 创建一个与人口密度数据相同形状的数组，用于存储每个像素的面积\n",
    "        pixel_area = np.zeros(population_density.shape)\n",
    "\n",
    "        # 计算每个像素的面积（单位：平方公里）\n",
    "        for i in range(pixel_area.shape[0]):\n",
    "            lat = dataset.xy(i, 0)[1]  # 获取纬度\n",
    "            pixel_height_km = 111.32 * 0.25  # 每个像素的高度（公里）\n",
    "            pixel_width_km = 111.32 * np.cos(np.deg2rad(lat)) * 0.25  # 每个像素的宽度（公里）\n",
    "            pixel_area[i, :] = pixel_height_km * pixel_width_km\n",
    "\n",
    "        # 计算人口数量：人口密度 * 像素面积\n",
    "        population_count = np.where(population_density != nodata, population_density * pixel_area, nodata)\n",
    "\n",
    "        # 将人口数量数据类型转换为 float32\n",
    "        population_count = population_count.astype('float32')\n",
    "\n",
    "        # 保存人口数量数据\n",
    "        with rasterio.open(output_file, 'w', driver='GTiff', height=dataset.height,\n",
    "                           width=dataset.width, count=1,\n",
    "                           dtype='float32', crs=dataset.crs, transform=dataset.transform, nodata=nodata) as dst:\n",
    "            dst.write(population_count, 1)\n",
    "\n",
    "    print(f\"Population count file saved: {output_file}\")\n",
    "\n",
    "# 新的处理逻辑\n",
    "years = range(2010, 2101, 10)\n",
    "ssps = ['ssp1', 'ssp2', 'ssp3', 'ssp5']\n",
    "input_base_path = \"L:/popdenRES/popdynamics-1-8th-pop-base-year-projection-ssp-2000-2100-rev01-proj-\"\n",
    "output_base_path = \"L:/popdenREScou/popdynamics-1-8th-pop-base-year-projection-ssp-2000-2100-rev01-proj-\"\n",
    "\n",
    "# 对每个SSP和年份的数据计算人口数量\n",
    "for ssp in ssps:\n",
    "    for year in years:\n",
    "        input_file = f\"{input_base_path}{ssp}-geotiff/{ssp}/Total/GeoTIFF/{ssp}_{year}denRES.tif\"\n",
    "        output_file = f\"{output_base_path}{ssp}-geotiff/{ssp}/Total/GeoTIFF/{ssp}_{year}denREScou.tif\"\n",
    "        calculate_population_count(input_file, output_file)\n",
    "        print(f\"Population count calculated for {ssp}, year: {year}\")\n",
    "\n",
    "print(\"All population count calculations are completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#将历史数据1990年人口密度数据转变为人口数量数据，可用  偏差不对  原始数据是5289191936.0，重采样为5302173696.0，后者是前者1.002454400062561"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def calculate_population_count(input_file, output_file):\n",
    "    # 检查输出文件路径是否存在，如果不存在则创建\n",
    "    output_dir = os.path.dirname(output_file)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    with rasterio.open(input_file) as dataset:\n",
    "        # 获取原始数据的 nodata 值\n",
    "        nodata = dataset.nodata\n",
    "\n",
    "        # 读取原始人口密度数据\n",
    "        population_density = dataset.read(1)\n",
    "\n",
    "        # 创建一个与人口密度数据相同形状的数组，用于存储每个像素的面积\n",
    "        pixel_area = np.zeros(population_density.shape)\n",
    "\n",
    "        # 计算每个像素的面积（单位：平方公里）\n",
    "        for i in range(pixel_area.shape[0]):\n",
    "            lat = dataset.xy(i, 0)[1]  # 获取纬度\n",
    "            pixel_height_km = 111.32 * 0.25  # 每个像素的高度（公里）\n",
    "            pixel_width_km = 111.32 * np.cos(np.deg2rad(lat)) * 0.25  # 每个像素的宽度（公里）\n",
    "            pixel_area[i, :] = pixel_height_km * pixel_width_km\n",
    "\n",
    "        # 计算人口数量：人口密度 * 像素面积\n",
    "        population_count = np.where(population_density != nodata, population_density * pixel_area, nodata)\n",
    "\n",
    "        # 将人口数量数据类型转换为 float32\n",
    "        population_count = population_count.astype('float32')\n",
    "\n",
    "        # 保存人口数量数据\n",
    "        with rasterio.open(output_file, 'w', driver='GTiff', height=dataset.height,\n",
    "                           width=dataset.width, count=1,\n",
    "                           dtype='float32', crs=dataset.crs, transform=dataset.transform, nodata=nodata) as dst:\n",
    "            dst.write(population_count, 1)\n",
    "\n",
    "    print(f\"Population count file saved: {output_file}\")\n",
    "\n",
    "# 执行计算人口数量的函数\n",
    "input_file = \"L:/popdenRES/popdynamics-global-pop-count-time-series-estimates-1990-geotiff/popdynamics-global-pop-count-time-series-estimates_1990denRES.tif\"\n",
    "output_file = \"L:/popdenREScou/popdynamics-global-pop-count-time-series-estimates-1990-geotiff/popdynamics-global-pop-count-time-series-estimates_1990denREScou.tif\"\n",
    "\n",
    "calculate_population_count(input_file, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#重要！计算原始数据人口数量和重采样后的人口数量及两者比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "def calculate_population_sum(file_path):\n",
    "    try:\n",
    "        with rasterio.open(file_path) as src:\n",
    "            data = src.read(1)  # 读取第一个波段\n",
    "\n",
    "            # 处理 nodata 值和nan值，将它们替换为0\n",
    "            nodata = src.nodata\n",
    "            if nodata is not None:\n",
    "                data[data == nodata] = 0\n",
    "            data = np.nan_to_num(data)\n",
    "\n",
    "            # 计算总和\n",
    "            total_population = np.sum(data)\n",
    "        return total_population\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "# 请替换为实际文件路径\n",
    "file_path1 = 'L:/pop/gpw-v4-population-count-rev11_2020_15_min_tif/gpw_v4_population_count_rev11_2020_15_min.tif'\n",
    "file_path2 = 'L:/popdenREScou/gpw-v4-population-count-rev11_2020_15_min_tif/gpw_v4_population_count_rev11_2020_15_min.tif'\n",
    "\n",
    "# 计算人口总和\n",
    "population_sum1 = calculate_population_sum(file_path1)\n",
    "population_sum2 = calculate_population_sum(file_path2)\n",
    "\n",
    "# 计算比值\n",
    "ratio = population_sum2 / population_sum1 if population_sum1 != 0 else None\n",
    "\n",
    "print(f\"Population sum for {file_path1}: {population_sum1}\")\n",
    "print(f\"Population sum for {file_path2}: {population_sum2}\")\n",
    "print(f\"Ratio of population sums: {ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#计算出中间年份数据，比如基于2010年和2020年数据计算出2015年数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def calculate_intermediate_population(input_file_early, input_file_later, output_file_intermediate):\n",
    "    # 检查输出文件路径是否存在，如果不存在则创建\n",
    "    output_dir = os.path.dirname(output_file_intermediate)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    with rasterio.open(input_file_early) as early_data, rasterio.open(input_file_later) as later_data:\n",
    "        # 读取数据\n",
    "        pop_early = early_data.read(1)\n",
    "        pop_later = later_data.read(1)\n",
    "\n",
    "        # 获取nodata值\n",
    "        nodata_early = early_data.nodata\n",
    "        nodata_later = later_data.nodata\n",
    "\n",
    "        # 初始化中间年份的人口数量数组，并设置初始值为nodata\n",
    "        pop_intermediate = np.full(pop_early.shape, nodata_early, dtype=np.float32)\n",
    "\n",
    "        # 只有当两个年份都不是Nodata时，计算平均值\n",
    "        mask_average = (pop_early != nodata_early) & (pop_later != nodata_later)\n",
    "        pop_intermediate[mask_average] = (pop_early[mask_average] + pop_later[mask_average]) / 2\n",
    "\n",
    "        # 如果一个年份是Nodata而另一个不是，则使用非Nodata年份的0.5倍\n",
    "        mask_half_early = (pop_early != nodata_early) & (pop_later == nodata_later)\n",
    "        pop_intermediate[mask_half_early] = pop_early[mask_half_early] * 0.5\n",
    "        mask_half_later = (pop_early == nodata_early) & (pop_later != nodata_later)\n",
    "        pop_intermediate[mask_half_later] = pop_later[mask_half_later] * 0.5\n",
    "\n",
    "        # 使用与早期数据相同的元数据保存结果\n",
    "        with rasterio.open(output_file_intermediate, 'w', **early_data.meta) as dst:\n",
    "            dst.write(pop_intermediate, 1)\n",
    "\n",
    "    print(f\"File saved: {output_file_intermediate}\")\n",
    "\n",
    "# 新的处理逻辑\n",
    "ssps = ['ssp1', 'ssp2', 'ssp3', 'ssp5']\n",
    "base_path = \"L:/popdenREScou/popdynamics-1-8th-pop-base-year-projection-ssp-2000-2100-rev01-proj-\"\n",
    "years = range(2010, 2091, 10)  # 修改至2091以包含2090\n",
    "\n",
    "# 对每个SSP和每个十年间隔的数据进行处理\n",
    "for ssp in ssps:\n",
    "    for year in years:\n",
    "        input_file_early = f\"{base_path}{ssp}-geotiff/{ssp}/Total/GeoTIFF/{ssp}_{year}denREScou.tif\"\n",
    "        input_file_later = f\"{base_path}{ssp}-geotiff/{ssp}/Total/GeoTIFF/{ssp}_{year+10}denREScou.tif\"\n",
    "        output_file_intermediate = f\"{base_path}{ssp}-geotiff/{ssp}/Total/GeoTIFF/{ssp}_{year+5}denREScou.tif\"\n",
    "        calculate_intermediate_population(input_file_early, input_file_later, output_file_intermediate)\n",
    "\n",
    "print(\"All intermediate population calculations are completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#气温数据和人口数据空间不匹配，比如气温数据空间范围extent的top为90，left为0，right为360，bottom为-60，\n",
    "#但人口数据空间范围extent的top是83.74999999，left为-180，right为180，bottom为-55.750000001\n",
    "#所以需要先基于气温数据对人口数据进行重采样\n",
    "#new_width = 1440  # 按照气温数据的尺寸调整\n",
    "#new_height = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.warp import reproject, Resampling\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 定义基础文件路径\n",
    "base_tif_path = 'L:\\\\popdenREScou\\\\'\n",
    "base_output_path = 'L:\\\\popdenREScou\\\\RE_{}\\\\Total\\\\GeoTIFF\\\\'  # {} 用于插入不同的情景\n",
    "\n",
    "# 定义情景和年份\n",
    "scenarios = ['ssp1', 'ssp2', 'ssp3', 'ssp5']\n",
    "years = range(2010, 2101, 5)  # 从2010到2100年，每隔五年\n",
    "\n",
    "# 重采样参数\n",
    "dst_crs = 'EPSG:4326'\n",
    "new_width = 1440  # 按照气温数据的尺寸调整\n",
    "new_height = 600\n",
    "\n",
    "for scenario in scenarios:\n",
    "    for year in years:\n",
    "        tif_file = f'{base_tif_path}{scenario}\\\\Total\\\\GeoTIFF\\\\{scenario}_{year}denREScou.tif'\n",
    "        output_file = f'{base_output_path.format(scenario)}{scenario}_{year}denREScou_RE.tif'\n",
    "\n",
    "        # 确保输出文件的目录存在\n",
    "        output_dir = os.path.dirname(output_file)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        with rasterio.open(tif_file) as src:\n",
    "            nodata = src.nodata\n",
    "            original_transform = src.transform\n",
    "            transform = rasterio.transform.from_bounds(0, -60, 360, 90, new_width, new_height)\n",
    "\n",
    "            # 更新文件profile并重采样\n",
    "            profile = src.profile\n",
    "            profile.update(transform=transform, crs=dst_crs, width=new_width, height=new_height)\n",
    "            \n",
    "            with rasterio.open(output_file, 'w', **profile) as dst:\n",
    "                reproject(\n",
    "                    source=rasterio.band(src, 1),\n",
    "                    destination=rasterio.band(dst, 1),\n",
    "                    src_transform=original_transform,\n",
    "                    src_crs=src.crs,\n",
    "                    dst_transform=transform,\n",
    "                    dst_crs=dst_crs,\n",
    "                    resampling=Resampling.nearest)\n",
    "\n",
    "        # 以读取模式打开重采样后的数据集\n",
    "        with rasterio.open(output_file, 'r+') as dst:\n",
    "            dst_data = dst.read(1)\n",
    "            dst_data[dst_data == nodata] = np.nan\n",
    "            dst.write_band(1, dst_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#这里发现历史人口数据L:\\popdenREScou\\RE_gpw-v4-population-count-rev11_1990_15_min_tif\\gpw_v4_population_count_rev11_1990_15_min_RE.tif和未来人口数据L:\\popdenREScou\\RE_ssp1\\Total\\GeoTIFF\\ssp1_2025denREScou_RE.tif在形状和分辨率以及nodata上具有较大差异，将导致最后计算结果的错误，为此参考2025年数据情况对历史数据1990-2020年数据进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 文件夹路径\n",
    "input_folder_base = r'L:\\popdenREScou\\RE_gpw-v4-population-count-rev11_{}_15_min_tif'\n",
    "input_file_base = 'gpw_v4_population_count_rev11_{}_15_min_RE.tif'\n",
    "input_folder_2025 = r'L:\\popdenREScou\\RE_ssp1\\Total\\GeoTIFF'\n",
    "output_folder = r'L:\\popdenREScou\\RE_lishi2000_2020'\n",
    "\n",
    "# 确保输出文件夹存在，如果不存在则创建它\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 输入文件名\n",
    "input_file_2025 = 'ssp1_2025denREScou_RE.tif'\n",
    "\n",
    "for year in range(1990, 2021, 5):\n",
    "    input_folder = input_folder_base.format(year)\n",
    "    input_file = input_file_base.format(year)\n",
    "    output_file = os.path.join(output_folder, input_file_base.format(year))\n",
    "\n",
    "    # 打开当前年份的人口数据\n",
    "    with rasterio.open(os.path.join(input_folder, input_file)) as src1:\n",
    "        profile1 = src1.profile\n",
    "        data1 = src1.read(1)\n",
    "\n",
    "    # 打开2025年的人口数据\n",
    "    with rasterio.open(os.path.join(input_folder_2025, input_file_2025)) as src2:\n",
    "        profile2 = src2.profile\n",
    "        data2 = src2.read(1)\n",
    "\n",
    "    # 确保两份数据具有相同的形状和分辨率\n",
    "    assert profile1['width'] == profile2['width']\n",
    "    assert profile1['height'] == profile2['height']\n",
    "    assert profile1['transform'] == profile2['transform']\n",
    "\n",
    "    # 对齐两份数据\n",
    "    aligned_data = np.where(data1 != src1.nodata, data1, data2)\n",
    "\n",
    "    # 将对齐后的数据保存到新的TIFF文件\n",
    "    profile1.update(count=1, nodata=np.nan)\n",
    "    with rasterio.open(output_file, 'w', **profile1) as dst:\n",
    "        dst.write(aligned_data, 1)\n",
    "\n",
    "    print(f\"新的TIFF文件已成功保存至 {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#对历史数据进行处理2000-2020年，这里用的是上一步根据L:\\popdenREScou\\RE_ssp1\\Total\\GeoTIFF\\ssp1_2025denREScou_RE.tif新生成的数据L:\\popdenREScou\\RE_lishi2000_2020\\gpw_v4_population_count_rev11_1990_15_min_RE.tif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import calendar\n",
    "\n",
    "def is_leap_year(year):\n",
    "    \"\"\"判断是否是闰年\"\"\"\n",
    "    return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)\n",
    "\n",
    "def process_data(weather_scenario, population_scenario, start_year, end_year):\n",
    "    threshold_file = 'L:/CMIP6-1yuzhi/hi_yuzhi_day_r1i1p1f1_gn_2000_canshu0.1.nc'\n",
    "    threshold_data = xr.open_dataset(threshold_file).load()  # 预先加载阈值数据到内存\n",
    "\n",
    "    for year in range(start_year, end_year + 1, 5):\n",
    "        population_year = year - year % 5 if year % 5 <= 2 else year + (5 - year % 5)\n",
    "        weather_file = f'L:/CMIP6-1NEW/{weather_scenario}/hi_day_{weather_scenario}_r1i1p1f1_gn_{year}.nc'\n",
    "        population_file = f'L:/popdenREScou/RE_lishi2000_2020/gpw_v4_population_count_rev11_{population_year}_15_min_RE.tif'        \n",
    "        \n",
    "        weather_data = xr.open_dataset(weather_file).load()\n",
    "        with rasterio.open(population_file) as src:\n",
    "            population_array = src.read(1)\n",
    "            population_data = xr.DataArray(\n",
    "                population_array,\n",
    "                dims=[\"lat\", \"lon\"],\n",
    "                coords={\n",
    "                    \"lat\": np.linspace(src.bounds.top, src.bounds.bottom, src.height),\n",
    "                    \"lon\": np.linspace(src.bounds.left, src.bounds.right, src.width)\n",
    "                }\n",
    "            )\n",
    "            population_data = population_data.interp(lat=weather_data.lat, lon=weather_data.lon, method='nearest')\n",
    "\n",
    "        # 为了避免直接更新带有时间标签的DataArray，我们创建一个新的DataArray来存储调整后的数据\n",
    "        adjusted_weather_data = weather_data.copy(deep=True)\n",
    "        adjusted_weather_data['__xarray_dataarray_variable__'][:] = 0  # 初始化为0\n",
    "\n",
    "        # 处理闰年数据\n",
    "        if is_leap_year(year):\n",
    "            date_range = pd.date_range(f'{year}-01-01', f'{year}-12-31', freq='D') + pd.DateOffset(hours=12)\n",
    "        else:\n",
    "            date_range = pd.date_range(f'{year}-01-01', f'{year}-12-31', freq='D') + pd.DateOffset(hours=12)\n",
    "\n",
    "        # 调整阈值数据的时间维度以匹配目标年份\n",
    "        threshold_data_adj = threshold_data.reindex(time=date_range, method='nearest')\n",
    "\n",
    "        # 计算阈值调整后的数据\n",
    "        adjusted_weather_data['__xarray_dataarray_variable__'] = xr.where(\n",
    "            weather_data['__xarray_dataarray_variable__'] - threshold_data_adj['__xarray_dataarray_variable__'] > 0,\n",
    "            weather_data['__xarray_dataarray_variable__'] - threshold_data_adj['__xarray_dataarray_variable__'],\n",
    "            0\n",
    "        )\n",
    "\n",
    "        # 计算最终结果\n",
    "        result_data = adjusted_weather_data['__xarray_dataarray_variable__'] * population_data\n",
    "\n",
    "        # 保存结果\n",
    "        save_path = f'L:/jisuan18.3_0.1/{weather_scenario}/'\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        output_file_path = os.path.join(save_path, f'{weather_scenario}_{year}.nc')\n",
    "        result_data.to_netcdf(output_file_path)\n",
    "        print(f\"文件已成功保存至 {output_file_path}\")\n",
    "\n",
    "scenarios = {'ssp126': 'ssp1', 'ssp245': 'ssp2', 'ssp370': 'ssp3', 'ssp585': 'ssp5'}\n",
    "start_year = 2000\n",
    "end_year = 2020\n",
    "\n",
    "for weather_scenario, population_scenario in scenarios.items():\n",
    "    process_data(weather_scenario, population_scenario, start_year, end_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#检查气温数据和人口数据空间是否匹配\n",
    "#结果是\n",
    "#气温数据维度: OrderedDict([('time', <class 'netCDF4._netCDF4.Dimension'> (unlimited): name = 'time', size = 360), ('lon', <class 'netCDF4._netCDF4.Dimension'>: name = 'lon', size = 1440), ('lat', <class 'netCDF4._netCDF4.Dimension'>: name = 'lat', size = 600)])\n",
    "#气温数据经度范围: 0.125 359.875\n",
    "#气温数据纬度范围: -59.875 89.875\n",
    "#人口数据边界: BoundingBox(left=0.0, bottom=-60.0, right=360.0, top=90.0)\n",
    "#人口数据变换矩阵: | 0.25, 0.00, 0.00|\n",
    "#| 0.00,-0.27, 90.00|\n",
    "#| 0.00, 0.00, 1.00|\n",
    "#人口数据CRS: WGS 84\n",
    "#数据集在空间上是兼容的，可以相乘。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from netCDF4 import Dataset\n",
    "\n",
    "# 定义文件路径\n",
    "nc_file = 'L:/CMIP6-1/ssp126/tas_day_ssp126_r1i1p1f1_gn_2025.nc'\n",
    "tif_file = 'L:\\\\popdenREScou\\\\RE_ssp1\\\\Total\\\\GeoTIFF\\\\ssp1_2025denREScou_RE.tif'\n",
    "\n",
    "# 检查气温数据\n",
    "with Dataset(nc_file, 'r') as nc:\n",
    "    nc_dims = nc.dimensions\n",
    "    nc_vars = nc.variables\n",
    "    print(\"气温数据维度:\", nc_dims)\n",
    "    if 'lon' in nc_vars and 'lat' in nc_vars:\n",
    "        lon = nc_vars['lon'][:]\n",
    "        lat = nc_vars['lat'][:]\n",
    "        print(\"气温数据经度范围:\", lon.min(), lon.max())\n",
    "        print(\"气温数据纬度范围:\", lat.min(), lat.max())\n",
    "\n",
    "# 检查人口数据\n",
    "with rasterio.open(tif_file) as src:\n",
    "    tif_bounds = src.bounds\n",
    "    tif_transform = src.transform\n",
    "    print(\"人口数据边界:\", tif_bounds)\n",
    "    print(\"人口数据变换矩阵:\", tif_transform)\n",
    "\n",
    "    # 检查CRS是否匹配\n",
    "    if src.crs.to_string() == 'EPSG:4326':\n",
    "        print(\"人口数据CRS: WGS 84\")\n",
    "    else:\n",
    "        print(\"人口数据CRS:\", src.crs)\n",
    "\n",
    "# 对比两个数据集以判断是否可以相乘\n",
    "# 这仅仅是一个简单的对比，可能需要更详细的分析\n",
    "if lon.min() >= tif_bounds.left and lon.max() <= tif_bounds.right and \\\n",
    "   lat.min() >= tif_bounds.bottom and lat.max() <= tif_bounds.top:\n",
    "    print(\"数据集在空间上是兼容的，可以相乘。\")\n",
    "else:\n",
    "    print(\"数据集在空间上不兼容，无法直接相乘。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#计算气温数据和人口数据乘积-预测数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "def process_data(weather_scenario, population_scenario, start_year, end_year):\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        # 根据年份确定人口数据年份\n",
    "        population_year = year - year % 5 if year % 5 <= 2 else year + (5 - year % 5)\n",
    "\n",
    "        # 构建气象和人口数据的文件路径\n",
    "        weather_file = f'L:/CMIP6-1new/{weather_scenario}/tas_day_{weather_scenario}_r1i1p1f1_gn_{year}.nc'\n",
    "        population_file = f'L:/popdenREScou/RE_{population_scenario}/Total/GeoTIFF/{population_scenario}_{population_year}denREScou_RE.tif'\n",
    "\n",
    "        # 读取气象数据\n",
    "        weather_data = xr.open_dataset(weather_file)\n",
    "\n",
    "        # 使用rasterio读取人口数据\n",
    "        with rasterio.open(population_file) as src:\n",
    "            population_array = src.read(1)\n",
    "            population_data = xr.DataArray(\n",
    "                population_array,\n",
    "                dims=[\"y\", \"x\"],\n",
    "                coords={\n",
    "                    \"y\": np.linspace(src.bounds.top, src.bounds.bottom, src.height),\n",
    "                    \"x\": np.linspace(src.bounds.left, src.bounds.right, src.width)\n",
    "                }\n",
    "            )\n",
    "            population_data = population_data.interp(y=weather_data['lat'], x=weather_data['lon'], method='nearest')\n",
    "\n",
    "        # 调整气象数据\n",
    "        adjusted_weather_data = weather_data['tas'] - 293.15\n",
    "        adjusted_weather_data = adjusted_weather_data.clip(min=0)\n",
    "\n",
    "        # 计算乘积\n",
    "        result_data = adjusted_weather_data * population_data\n",
    "\n",
    "        # 确保目标文件夹存在\n",
    "        save_path = f'L:/jisuan20/{weather_scenario}/'\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "        # 保存结果\n",
    "        result_data.to_netcdf(f'{save_path}{weather_scenario}_{year}.nc')\n",
    "\n",
    "# 循环处理每个情景和年份\n",
    "scenarios = {'ssp126': 'ssp1', 'ssp245': 'ssp2', 'ssp370': 'ssp3', 'ssp585': 'ssp5'}\n",
    "start_year = 2015\n",
    "end_year = 2100\n",
    "\n",
    "# 记录开始时间\n",
    "start_time = time.time()\n",
    "\n",
    "for weather_scenario, population_scenario in scenarios.items():\n",
    "    process_data(weather_scenario, population_scenario, start_year, end_year)\n",
    "\n",
    "# 计算并打印总运行时间\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "hours, rem = divmod(total_time, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(f\"Total time: {int(hours)} hours, {int(minutes)} minutes, {int(seconds)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#修改！计算气温数据和人口数据乘积时 不用固定阈值，而用新阈值 hi_yuzhi_day_r1i1p1f1_gn_2000_canshu0.1.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import calendar\n",
    "\n",
    "def is_leap_year(year):\n",
    "    \"\"\"判断是否是闰年\"\"\"\n",
    "    return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)\n",
    "\n",
    "def process_data(weather_scenario, population_scenario, start_year, end_year):\n",
    "    threshold_file = 'L:/CMIP6-1yuzhi/hi_yuzhi_day_r1i1p1f1_gn_2000_canshu0.1.nc'\n",
    "    threshold_data = xr.open_dataset(threshold_file).load()  # 预先加载阈值数据到内存\n",
    "\n",
    "    for year in range(start_year, end_year + 1, 5):\n",
    "        population_year = year - year % 5 if year % 5 <= 2 else year + (5 - year % 5)\n",
    "        weather_file = f'L:/CMIP6-1NEW/{weather_scenario}/hi_day_{weather_scenario}_r1i1p1f1_gn_{year}.nc'\n",
    "        population_file = f'L:/popdenREScou/RE_{population_scenario}/Total/GeoTIFF/{population_scenario}_{population_year}denREScou_RE.tif'\n",
    "\n",
    "        weather_data = xr.open_dataset(weather_file).load()\n",
    "        with rasterio.open(population_file) as src:\n",
    "            population_array = src.read(1)\n",
    "            population_data = xr.DataArray(\n",
    "                population_array,\n",
    "                dims=[\"lat\", \"lon\"],\n",
    "                coords={\n",
    "                    \"lat\": np.linspace(src.bounds.top, src.bounds.bottom, src.height),\n",
    "                    \"lon\": np.linspace(src.bounds.left, src.bounds.right, src.width)\n",
    "                }\n",
    "            )\n",
    "            population_data = population_data.interp(lat=weather_data.lat, lon=weather_data.lon, method='nearest')\n",
    "\n",
    "        # 为了避免直接更新带有时间标签的DataArray，我们创建一个新的DataArray来存储调整后的数据\n",
    "        adjusted_weather_data = weather_data.copy(deep=True)\n",
    "        adjusted_weather_data['__xarray_dataarray_variable__'][:] = 0  # 初始化为0\n",
    "\n",
    "        # 处理闰年数据\n",
    "        if is_leap_year(year):\n",
    "            date_range = pd.date_range(f'{year}-01-01', f'{year}-12-31', freq='D') + pd.DateOffset(hours=12)\n",
    "        else:\n",
    "            date_range = pd.date_range(f'{year}-01-01', f'{year}-12-31', freq='D') + pd.DateOffset(hours=12)\n",
    "\n",
    "        # 调整阈值数据的时间维度以匹配目标年份\n",
    "        threshold_data_adj = threshold_data.reindex(time=date_range, method='nearest')\n",
    "\n",
    "        # 计算阈值调整后的数据\n",
    "        adjusted_weather_data['__xarray_dataarray_variable__'] = xr.where(\n",
    "            weather_data['__xarray_dataarray_variable__'] - threshold_data_adj['__xarray_dataarray_variable__'] > 0,\n",
    "            weather_data['__xarray_dataarray_variable__'] - threshold_data_adj['__xarray_dataarray_variable__'],\n",
    "            0\n",
    "        )\n",
    "\n",
    "        # 计算最终结果\n",
    "        result_data = adjusted_weather_data['__xarray_dataarray_variable__'] * population_data\n",
    "\n",
    "        # 保存结果\n",
    "        save_path = f'L:/jisuan18.3_0.1/{weather_scenario}/'\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        output_file_path = os.path.join(save_path, f'{weather_scenario}_{year}.nc')\n",
    "        result_data.to_netcdf(output_file_path)\n",
    "        print(f\"文件已成功保存至 {output_file_path}\")\n",
    "\n",
    "scenarios = {'ssp126': 'ssp1', 'ssp245': 'ssp2', 'ssp370': 'ssp3', 'ssp585': 'ssp5'}\n",
    "start_year = 2025\n",
    "end_year = 2100\n",
    "\n",
    "for weather_scenario, population_scenario in scenarios.items():\n",
    "    process_data(weather_scenario, population_scenario, start_year, end_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#计算气温数据和人口数据乘积-历史数据\n",
    "#历史数据的相乘比预测数据的相乘更为复杂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "def process_data(weather_file, population_file, output_file):\n",
    "    # 读取气象数据\n",
    "    weather_data = xr.open_dataset(weather_file)\n",
    "\n",
    "    # 使用rasterio读取人口数据\n",
    "    with rasterio.open(population_file) as src:\n",
    "        population_array = src.read(1)\n",
    "        \n",
    "        # 处理nodata值，如果存在\n",
    "        nodata = src.nodata\n",
    "        if nodata is not None:\n",
    "            population_array[population_array == nodata] = np.nan\n",
    "\n",
    "        population_data = xr.DataArray(\n",
    "            population_array,\n",
    "            dims=[\"y\", \"x\"],\n",
    "            coords={\n",
    "                \"y\": np.linspace(src.bounds.top, src.bounds.bottom, src.height),\n",
    "                \"x\": np.linspace(src.bounds.left, src.bounds.right, src.width)\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # 与气象数据中的坐标对齐\n",
    "        population_data = population_data.interp(y=weather_data['lat'], x=weather_data['lon'], method='nearest')\n",
    "\n",
    "    # 调整气象数据\n",
    "    adjusted_weather_data = weather_data['tas'] - 293.15\n",
    "    adjusted_weather_data = adjusted_weather_data.clip(min=0)\n",
    "\n",
    "    # 将无穷值转换为0，但保留NaN\n",
    "    adjusted_weather_data = xr.where(np.isinf(adjusted_weather_data), 0, adjusted_weather_data)\n",
    "    population_data = xr.where(np.isinf(population_data), 0, population_data)\n",
    "\n",
    "    # 计算乘积\n",
    "    result_data = adjusted_weather_data * population_data\n",
    "\n",
    "    # 确保目标文件夹存在\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "    # 保存结果\n",
    "    result_data.to_netcdf(output_file)\n",
    "\n",
    "# 记录开始时间\n",
    "start_time = time.time()\n",
    "\n",
    "# 处理2000-2014年的气温数据和对应年份的人口数据\n",
    "for year in range(1990, 2015):\n",
    "    # 根据气温数据年份确定人口数据年份\n",
    "    population_year = year - year % 5 if year % 5 <= 2 else year + (5 - year % 5)\n",
    "\n",
    "    # 设置文件路径\n",
    "    weather_file = f'L:/CMIP6-1new/historical/tas_day_historical_r1i1p1f1_gn_{year}.nc'\n",
    "    population_file = f'L:/popdenREScou/RE_gpw-v4-population-count-rev11_{population_year}_15_min_tif/gpw_v4_population_count_rev11_{population_year}_15_min_RE.tif'\n",
    "    output_file = f'L:/jisuan20/historical/{year}.nc'\n",
    "\n",
    "    # 处理数据\n",
    "    process_data(weather_file, population_file, output_file)\n",
    "\n",
    "# 计算并打印总运行时间\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "hours, rem = divmod(total_time, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(f\"Total time: {int(hours)} hours, {int(minutes)} minutes, {int(seconds)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#这里发现历史人口数据L:\\popdenREScou\\RE_gpw-v4-population-count-rev11_1990_15_min_tif\\gpw_v4_population_count_rev11_1990_15_min_RE.tif和未来人口数据L:\\popdenREScou\\RE_ssp1\\Total\\GeoTIFF\\ssp1_2025denREScou_RE.tif在形状和分辨率以及nodata上具有较大差异，将导致最后计算结果的错误，为此参考2025年数据情况对历史数据1990-2020年数据进行处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 文件夹路径\n",
    "input_folder_base = r'L:\\popdenREScou\\RE_gpw-v4-population-count-rev11_{}_15_min_tif'\n",
    "input_file_base = 'gpw_v4_population_count_rev11_{}_15_min_RE.tif'\n",
    "input_folder_2025 = r'L:\\popdenREScou\\RE_ssp1\\Total\\GeoTIFF'\n",
    "output_folder = r'L:\\popdenREScou\\RE_lishi2000_2020'\n",
    "\n",
    "# 确保输出文件夹存在，如果不存在则创建它\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 输入文件名\n",
    "input_file_2025 = 'ssp1_2025denREScou_RE.tif'\n",
    "\n",
    "for year in range(1990, 2021, 5):\n",
    "    input_folder = input_folder_base.format(year)\n",
    "    input_file = input_file_base.format(year)\n",
    "    output_file = os.path.join(output_folder, input_file_base.format(year))\n",
    "\n",
    "    # 打开当前年份的人口数据\n",
    "    with rasterio.open(os.path.join(input_folder, input_file)) as src1:\n",
    "        profile1 = src1.profile\n",
    "        data1 = src1.read(1)\n",
    "\n",
    "    # 打开2025年的人口数据\n",
    "    with rasterio.open(os.path.join(input_folder_2025, input_file_2025)) as src2:\n",
    "        profile2 = src2.profile\n",
    "        data2 = src2.read(1)\n",
    "\n",
    "    # 确保两份数据具有相同的形状和分辨率\n",
    "    assert profile1['width'] == profile2['width']\n",
    "    assert profile1['height'] == profile2['height']\n",
    "    assert profile1['transform'] == profile2['transform']\n",
    "\n",
    "    # 对齐两份数据\n",
    "    aligned_data = np.where(data1 != src1.nodata, data1, data2)\n",
    "\n",
    "    # 将对齐后的数据保存到新的TIFF文件\n",
    "    profile1.update(count=1, nodata=np.nan)\n",
    "    with rasterio.open(output_file, 'w', **profile1) as dst:\n",
    "        dst.write(aligned_data, 1)\n",
    "\n",
    "    print(f\"新的TIFF文件已成功保存至 {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#对历史数据进行处理2000-2020年，这里用的是上一步根据L:\\popdenREScou\\RE_ssp1\\Total\\GeoTIFF\\ssp1_2025denREScou_RE.tif新生成的数据L:\\popdenREScou\\RE_lishi2000_2020\\gpw_v4_population_count_rev11_1990_15_min_RE.tif\n",
    "#修改！计算气温数据和人口数据乘积时 不用固定阈值，而用新阈值 hi_yuzhi_day_r1i1p1f1_gn_2000_canshu0.1.nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import calendar\n",
    "\n",
    "def is_leap_year(year):\n",
    "    \"\"\"判断是否是闰年\"\"\"\n",
    "    return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)\n",
    "\n",
    "def process_data(weather_scenario, population_scenario, start_year, end_year):\n",
    "    threshold_file = 'L:/CMIP6-1yuzhi/hi_yuzhi_day_r1i1p1f1_gn_2000_canshu0.1.nc'\n",
    "    threshold_data = xr.open_dataset(threshold_file).load()  # 预先加载阈值数据到内存\n",
    "\n",
    "    for year in range(start_year, end_year + 1, 5):\n",
    "        population_year = year - year % 5 if year % 5 <= 2 else year + (5 - year % 5)\n",
    "        weather_file = f'L:/CMIP6-1NEW/{weather_scenario}/hi_day_{weather_scenario}_r1i1p1f1_gn_{year}.nc'\n",
    "        population_file = f'L:/popdenREScou/RE_lishi2000_2020/gpw_v4_population_count_rev11_{population_year}_15_min_RE.tif'        \n",
    "        \n",
    "        weather_data = xr.open_dataset(weather_file).load()\n",
    "        with rasterio.open(population_file) as src:\n",
    "            population_array = src.read(1)\n",
    "            population_data = xr.DataArray(\n",
    "                population_array,\n",
    "                dims=[\"lat\", \"lon\"],\n",
    "                coords={\n",
    "                    \"lat\": np.linspace(src.bounds.top, src.bounds.bottom, src.height),\n",
    "                    \"lon\": np.linspace(src.bounds.left, src.bounds.right, src.width)\n",
    "                }\n",
    "            )\n",
    "            population_data = population_data.interp(lat=weather_data.lat, lon=weather_data.lon, method='nearest')\n",
    "\n",
    "        # 为了避免直接更新带有时间标签的DataArray，我们创建一个新的DataArray来存储调整后的数据\n",
    "        adjusted_weather_data = weather_data.copy(deep=True)\n",
    "        adjusted_weather_data['__xarray_dataarray_variable__'][:] = 0  # 初始化为0\n",
    "\n",
    "        # 处理闰年数据\n",
    "        if is_leap_year(year):\n",
    "            date_range = pd.date_range(f'{year}-01-01', f'{year}-12-31', freq='D') + pd.DateOffset(hours=12)\n",
    "        else:\n",
    "            date_range = pd.date_range(f'{year}-01-01', f'{year}-12-31', freq='D') + pd.DateOffset(hours=12)\n",
    "\n",
    "        # 调整阈值数据的时间维度以匹配目标年份\n",
    "        threshold_data_adj = threshold_data.reindex(time=date_range, method='nearest')\n",
    "\n",
    "        # 计算阈值调整后的数据\n",
    "        adjusted_weather_data['__xarray_dataarray_variable__'] = xr.where(\n",
    "            weather_data['__xarray_dataarray_variable__'] - threshold_data_adj['__xarray_dataarray_variable__'] > 0,\n",
    "            weather_data['__xarray_dataarray_variable__'] - threshold_data_adj['__xarray_dataarray_variable__'],\n",
    "            0\n",
    "        )\n",
    "\n",
    "        # 计算最终结果\n",
    "        result_data = adjusted_weather_data['__xarray_dataarray_variable__'] * population_data\n",
    "\n",
    "        # 保存结果\n",
    "        save_path = f'L:/jisuan18.3_0.1/{weather_scenario}/'\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        output_file_path = os.path.join(save_path, f'{weather_scenario}_{year}.nc')\n",
    "        result_data.to_netcdf(output_file_path)\n",
    "        print(f\"文件已成功保存至 {output_file_path}\")\n",
    "\n",
    "scenarios = {'ssp126': 'ssp1', 'ssp245': 'ssp2', 'ssp370': 'ssp3', 'ssp585': 'ssp5'}\n",
    "start_year = 2000\n",
    "end_year = 2020\n",
    "\n",
    "for weather_scenario, population_scenario in scenarios.items():\n",
    "    process_data(weather_scenario, population_scenario, start_year, end_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#最后一步，基于国家shp面数据计算栅格数值和\n",
    "#这一步是不是不对啊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import Point\n",
    "import time\n",
    "\n",
    "def process_nc_data(nc_file, shp_file, output_excel):\n",
    "    print(\"正在读取气象数据...\")\n",
    "    data = xr.open_dataset(nc_file)\n",
    "    var_name = list(data.data_vars)[0]\n",
    "    variable_data = data[var_name]\n",
    "    \n",
    "    print(\"正在读取矢量地图并重投影...\")\n",
    "    countries = gpd.read_file(shp_file)\n",
    "    countries = countries.to_crs(epsg=4326)\n",
    "    \n",
    "    print(\"正在处理数据...\")\n",
    "    final_results = pd.DataFrame()\n",
    "\n",
    "    for time_step in variable_data.time:\n",
    "        subset = variable_data.sel(time=time_step).to_dataframe().reset_index()\n",
    "        points_gdf = gpd.GeoDataFrame(subset, geometry=gpd.points_from_xy(subset.lon, subset.lat))\n",
    "        points_gdf.crs = 'epsg:4326'\n",
    "        joined = gpd.sjoin(points_gdf, countries, how=\"inner\", op=\"within\")\n",
    "        \n",
    "        # 计算每个国家的栅格数据总和\n",
    "        country_sums = joined.groupby('FCNAME')[var_name].sum()\n",
    "        country_sums.name = pd.to_datetime(time_step.values).strftime('%Y-%m-%d')\n",
    "        final_results = final_results.append(country_sums)\n",
    "\n",
    "    print(\"数据处理完成。\")\n",
    "\n",
    "    print(\"正在格式化结果并保存为Excel...\")\n",
    "    final_results = final_results.T\n",
    "    final_results.to_excel(output_excel)\n",
    "\n",
    "    print(\"计算完成，结果已保存至\", output_excel)\n",
    "\n",
    "# 文件路径\n",
    "nc_file = \"L:/jisuan18.3/ssp126/ssp126_2030.nc\"\n",
    "shp_file = \"L:/gis底图/countryBN/countryBN.shp\"\n",
    "output_excel = \"L:/result18.3/ssp126/ssp126_2030.xlsx\"\n",
    "\n",
    "# 记录时间并运行函数\n",
    "start_time = time.time()\n",
    "process_nc_data(nc_file, shp_file, output_excel)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"总运行时间：{elapsed_time:.2f} 秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#对2010-2100年进行重采样处理，原来数据的空间精度为0.125度*0.125度，重采样后的数据精度为0.25度*0.25度\n",
    "#这是直接重采样的代码，现在不需要了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.enums import Resampling\n",
    "from rasterio.transform import from_origin\n",
    "import os\n",
    "\n",
    "def resample_tif(input_file, output_file, desired_cell_size):\n",
    "    # 检查输出文件路径是否存在，如果不存在则创建\n",
    "    output_dir = os.path.dirname(output_file)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    with rasterio.open(input_file) as dataset:\n",
    "        # 获取原始数据的 nodata 值\n",
    "        nodata = dataset.nodata\n",
    "\n",
    "        # 计算新的宽度和高度\n",
    "        new_width = int(dataset.width * (dataset.res[0] / desired_cell_size))\n",
    "        new_height = int(dataset.height * (dataset.res[1] / desired_cell_size))\n",
    "\n",
    "        # 调整分辨率\n",
    "        data = dataset.read(\n",
    "            out_shape=(\n",
    "                dataset.count,\n",
    "                new_height,\n",
    "                new_width\n",
    "            ),\n",
    "            resampling=Resampling.nearest  # 使用最近邻插值\n",
    "        )\n",
    "\n",
    "        # 创建新的变换参数，确保每个像素的尺寸为0.25度*0.25度\n",
    "        new_transform = from_origin(dataset.bounds.left, dataset.bounds.top, desired_cell_size, desired_cell_size)\n",
    "\n",
    "        # 保存重采样后的文件\n",
    "        with rasterio.open(output_file, 'w', driver='GTiff', height=new_height,\n",
    "                           width=new_width, count=dataset.count,\n",
    "                           dtype=data.dtype, crs=dataset.crs, transform=new_transform, nodata=nodata) as dst:\n",
    "            dst.write(data)\n",
    "\n",
    "    print(f\"File saved: {output_file}\")\n",
    "\n",
    "# 新的处理逻辑\n",
    "desired_cell_size = 0.25  # 欲达到的每个单元格的尺寸\n",
    "years = range(2010, 2101, 10)\n",
    "ssps = ['ssp1','ssp2', 'ssp3', 'ssp5']\n",
    "base_path = \"L:/pop/popdynamics-1-8th-pop-base-year-projection-ssp-2000-2100-rev01-proj-\"\n",
    "output_base_path = \"L:/popRES/popdynamics-1-8th-pop-base-year-projection-ssp-2000-2100-rev01-proj-\"\n",
    "\n",
    "# 对每个SSP和年份的数据进行重采样\n",
    "for ssp in ssps:\n",
    "    for year in years:\n",
    "        input_file = f\"{base_path}{ssp}-geotiff/{ssp}/Total/GeoTIFF/{ssp}_{year}.tif\"\n",
    "        output_file = f\"{output_base_path}{ssp}-geotiff/{ssp}/Total/GeoTIFF/{ssp}_{year}res.tif\"\n",
    "        resample_tif(input_file, output_file, desired_cell_size)\n",
    "        print(f\"Resampling completed for {ssp}, year: {year}\")\n",
    "\n",
    "print(\"All resampling processes are completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#基于世界各国shp文件计算国家尺度气温均值 #进一步做了优化，速度加快，且可以计算运行时间。#3.73小时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 开始计时\n",
    "start_time = time.time()\n",
    "\n",
    "# 文件路径\n",
    "nc_file = r'L:\\GDDP-CMIP6\\ACCESS-CM2\\historical\\tasmax_day_ACCESS-CM2_historical_r1i1p1f1_gn_1992.nc'\n",
    "shp_file = r'F:\\做图\\世界底图\\countryBN\\countryBN.shp'\n",
    "output_excel = r'L:\\GDDP-CMIP6\\ACCESS-CM2\\historical_1\\country_temperatures1992.xlsx'\n",
    "\n",
    "print(\"正在读取气象数据...\")\n",
    "data = xr.open_dataset(nc_file)\n",
    "tasmax = data.tasmax\n",
    "print(\"气象数据读取完成。\")\n",
    "\n",
    "print(\"正在读取矢量地图并重投影...\")\n",
    "countries = gpd.read_file(shp_file)\n",
    "countries = countries.to_crs(epsg=4326)\n",
    "country_name_column = 'FCNAME'\n",
    "countries.sort_values(country_name_column, inplace=True)\n",
    "print(\"矢量地图读取并重投影完成。\")\n",
    "\n",
    "print(\"正在处理数据...\")\n",
    "# 初始化一个空的 DataFrame 用于存储最终结果\n",
    "final_results = pd.DataFrame()\n",
    "\n",
    "# 对每个时间步长处理数据\n",
    "for time_step in tasmax.time:\n",
    "    tasmax_subset = tasmax.sel(time=time_step)\n",
    "    df_subset = tasmax_subset.to_dataframe(name='tasmax').reset_index()\n",
    "    points_gdf = gpd.GeoDataFrame(df_subset, geometry=gpd.points_from_xy(df_subset.lon, df_subset.lat))\n",
    "    points_gdf.crs = 'epsg:4326'\n",
    "    joined = gpd.sjoin(points_gdf, countries, how=\"inner\", op=\"within\")\n",
    "    avg_temps = joined.groupby(country_name_column)['tasmax'].mean()\n",
    "    avg_temps.name = pd.to_datetime(time_step.values).strftime('%Y%m%d')\n",
    "    final_results = final_results.append(avg_temps)\n",
    "\n",
    "print(\"数据处理完成。\")\n",
    "\n",
    "print(\"正在格式化结果并保存为Excel...\")\n",
    "# 转置 DataFrame，以便时间步长成为列\n",
    "final_results = final_results.T\n",
    "final_results.to_excel(output_excel)\n",
    "\n",
    "print(\"计算完成，结果已保存至\", output_excel)\n",
    "\n",
    "# 结束计时并计算总运行时间\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"总运行时间：{total_time:.2f} 秒\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#检查有问题的数据(历史)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
